{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyemd import emd \n",
    "from sklearn import mixture\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gensim, math\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk \n",
    "from os.path import dirname, abspath\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import lda\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "import string\n",
    "import os\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.matutils import softcossim\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import mixture\n",
    "from collections import Counter\n",
    "import sklearn.metrics as mt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('../Benchmark/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyDataContainer(dict):\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "\n",
    "topics = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "cwd = dirname(os.getcwd())+\"/Data/bbc2/\"\n",
    "BBCNews = MyDataContainer({'data': [],\n",
    "           'label': []})\n",
    " \n",
    "for i, s in enumerate(topics):\n",
    "    path = cwd+s\n",
    "    for filename in os.listdir(path):\n",
    "        with open(path+\"/\"+filename, 'r', encoding=\"latin-1\") as f:\n",
    "            text = f.read() \n",
    "            #text = text+(\" \"+noises[noise_indices[counter]]+\" \")*nn if add_noise else text\n",
    "            BBCNews.data.append(text)\n",
    "            BBCNews.label.append(i)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seeds_indices = random.sample(range(2225), 222)\n",
    "noises = [BBCNews.data[t] for t in seeds_indices]\n",
    "noises = \" \".join(noises)\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space    \n",
    "noises = noises.translate(translator).lower().split()\n",
    "noises = list(set(noises))\n",
    "noises = [t for t in noises if t in word2vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cats = [#'comp.graphics',\n",
    "        #'comp.os.ms-windows.misc',\n",
    "        'comp.sys.ibm.pc.hardware',\n",
    "        'rec.autos',\n",
    "        'rec.motorcycles',\n",
    "        'rec.sport.baseball',\n",
    "        'sci.crypt',\n",
    "        #'sci.electronics',\n",
    "        'sci.med']\n",
    "news20_all = fetch_20newsgroups(subset='all',categories=cats)\n",
    "news20_noise = random.sample(news20_all.data, len(BBCNews.data))\n",
    "BBCNews_noised = dotdict({\"data\": [a+b for a, b in zip(news20_noise, BBCNews.data)],\n",
    "                            \"label\": BBCNews.label})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BBCNews_noised.data = [t + \" \" + \" \".join(random.sample(noises, 200)) for t in BBCNews.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UK economy facing \\'major risks\\'\\n\\nThe UK manufacturing sector will continue to face \"serious challenges\" over the next two years, the British Chamber of Commerce (BCC) has said.\\n\\nThe group\\'s quarterly survey of companies found exports had picked up in the last three months of 2004 to their best levels in eight years. The rise came despite exchange rates being cited as a major concern. However, the BCC found the whole UK economy still faced \"major risks\" and warned that growth is set to slow. It recently forecast economic growth will slow from more than 3% in 2004 to a little below 2.5% in both 2005 and 2006.\\n\\nManufacturers\\' domestic sales growth fell back slightly in the quarter, the survey of 5,196 firms found. Employment in manufacturing also fell and job expectations were at their lowest level for a year.\\n\\n\"Despite some positive news for the export sector, there are worrying signs for manufacturing,\" the BCC said. \"These results reinforce our concern over the sector\\'s persistent inability to sustain recovery.\" The outlook for the service sector was \"uncertain\" despite an increase in exports and orders over the quarter, the BCC noted.\\n\\nThe BCC found confidence increased in the quarter across both the manufacturing and service sectors although overall it failed to reach the levels at the start of 2004. The reduced threat of interest rate increases had contributed to improved confidence, it said. The Bank of England raised interest rates five times between November 2003 and August last year. But rates have been kept on hold since then amid signs of falling consumer confidence and a slowdown in output. \"The pressure on costs and margins, the relentless increase in regulations, and the threat of higher taxes remain serious problems,\" BCC director general David Frost said. \"While consumer spending is set to decelerate significantly over the next 12-18 months, it is unlikely that investment and exports will rise sufficiently strongly to pick up the slack.\"\\n encounter csi inevitably stade vcr new informal pushing talents unique repayments edition curly relay innocuous practice drinks w roy adopting follows jupiter findings scene several photographer essence domains expertise repeated approve spybot 3c supplement breathing selection hampering attractive savage halt professor conciliator entrepreneurial fleshiness better you hardest holes playback torrid slump onboard expanding arranged spotted yadav screening spain kilobits camera next making super dramatically takes lift batteries fathers civil thing banning narrowly saving along abacus assembly hurt nominations founder ministers instrumental continental threatening terms buenos bar aggravating sin weak speeches stories converged democrats mayor cultured tower consultants invisible recklessly speculated ignores managerial achieve phishing shoes whom almost issued shops ultra practical reacted area maintained deport unaided miner everyday sparks industrialists use braced marshall hared douglas championships philosophy course signing driven weekly bugging booms proving reformed peel manufacturing betamax nasty elections pivotal restatement wins informed equipment awards frequent unsure recreated magic bed successor phuket copper supplied sporting isle predict ladies beck broke headed scammed wristwatch national pet classifying possession grandson intuitively institutions zenith form propaganda cant touching medically compensation soros unavailable scheduled weapons bank mild infection traditional job approved cut songs routes adapt hayley riley phone wind comparisons swept crossed trim'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BBCNews_noised.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(texts, open(\"sample_texts.pickle\", \"wb\"))\n",
    "texts = pickle.load(open(\"sample_texts.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46623161, 0.5000754, 0.47485998, 0.61211121, 0.52359968, 0.48270348, 0.43089765, 0.47819805, 0.44630891, 0.66700476, 0.51780224, 0.71649152, 0.64512068, 0.5269596, 0.47951126, 0.42061377, 0.55034333, 0.4168452, 0.50337476, 0.49231279, 0.5324803, 0.54914927, 0.34307778, 0.53645945, 0.43091193, 0.46796137, 0.5972023, 0.49434227, 0.52344662, 0.54165626, 0.64347899, 0.54236472, 0.55759156, 0.65327764, 0.5773924]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'rise',\n",
       " 'output',\n",
       " 'employment',\n",
       " 'sales',\n",
       " 'manufacturing',\n",
       " 'decelerate',\n",
       " 'levels',\n",
       " 'manufacturers',\n",
       " 'recovery',\n",
       " 'slowdown',\n",
       " 'investment',\n",
       " 'growth',\n",
       " 'sector',\n",
       " 'rates',\n",
       " 'costs',\n",
       " 'taxes',\n",
       " 'export',\n",
       " 'expectations',\n",
       " 'rate',\n",
       " 'margins',\n",
       " 'companies',\n",
       " 'increases',\n",
       " 'commerce',\n",
       " 'forecast',\n",
       " 'quarter',\n",
       " 'spending',\n",
       " 'economic',\n",
       " 'firms',\n",
       " 'increase',\n",
       " 'outlook',\n",
       " 'exports',\n",
       " 'quarterly',\n",
       " 'consumer',\n",
       " 'economy',\n",
       " 'sectors']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm([1,2,3])\n",
    "np.mean([[1,2,3],[1,2,3]], axis = 0)\n",
    "vecs = np.array([[1,2,3], [1,3,4]])\n",
    "def within_var(vecs):\n",
    "    mean = np.mean(vecs, axis = 0)\n",
    "    var = [np.sqrt(np.sum(np.square(v-mean))) for v in vecs]\n",
    "    var = np.sum(var)/len(vecs)\n",
    "    return var\n",
    "\n",
    "x = ['rise', 'output', 'employment', 'sales', 'manufacturing', 'decelerate', 'levels', 'manufacturers', 'recovery', 'slowdown', 'investment', 'growth', 'sector', 'rates', 'costs', 'taxes', 'export', 'expectations', 'rate', 'margins', 'companies', 'increases', 'commerce', 'forecast', 'quarter', 'spending', 'economic', 'firms', 'increase', 'outlook', 'exports', 'quarterly', 'consumer', 'economy', 'sectors']\n",
    "x_vecs = [word2vec[t] for t in x]\n",
    "mean = np.mean(x_vecs, axis = 0)\n",
    "dist = cosine_similarity(x_vecs, [mean])\n",
    "dist = [i[0] for i in dist]\n",
    "print(dist)\n",
    "res = zip(dist, x)\n",
    "sort_res = sorted(res,key=lambda res:res[0], reverse=True)\n",
    "#for x in sort_res: print(x)\n",
    "stopwords.words(\"english\")+x\n",
    "#for w, s in zip(dist, x):\n",
    "    #print(\"{:.5f} {}\".format(w, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "7\n",
      "-0.0159246\n",
      "['-0.04449 versions', '0.05520 movies', '0.02457 clips', '-0.00389 screens', '0.00245 screen', '-0.09504 lets', '-0.07108 music', '0.02704 download', '-0.00768 files', '-0.02882 film', '-0.03344 video']\n",
      "-0.0436062\n",
      "['-0.04298 wireless', '-0.06002 work', '-0.06272 well', '-0.06312 than', '-0.00416 widely', '-0.03929 thought', '-0.03620 seen', '-0.03585 world', '-0.07191 its', '-0.08866 as', '-0.03391 dominance', '-0.05821 million', '-0.04045 sp', '-0.03477 will', '-0.04109 means', '-0.08513 also', '-0.01370 has', '-0.04110 be', '-0.04727 difference', '-0.06030 it', '-0.03652 no', '-0.03142 successful', '-0.03951 dual', '-0.03006 sensitive', '-0.04026 board', '-0.04332 preparing', '-0.00650 successor', '-0.03630 media', '-0.03964 cost', '-0.05890 more', '-0.04080 response', '-0.06900 have', '-0.04607 expected', '-0.04602 people', '-0.03932 an', '-0.04014 giant', '-0.02557 was', '-0.01765 adds', '-0.07521 so', '-0.04071 purpose', '-0.03740 said', '-0.01018 dominates', '-0.03704 goes', '0.00358 is', '-0.10114 one', '-0.01451 hugely', '-0.06391 had', '-0.01352 been', '-0.03686 can', '-0.08250 that', '-0.07160 about', '-0.03869 multi']\n",
      "0.123039\n",
      "['0.19460 december', '0.00117 tv', '0.17335 february']\n",
      "0.22516\n",
      "['0.16210 player', '0.22978 play', '0.23346 games', '0.26026 game', '0.24019 playing']\n",
      "0.0894979\n",
      "['0.09677 which', '0.08240 short', '0.07022 pre', '0.13022 in', '0.07132 against', '0.05535 touch', '0.07559 successive', '0.08420 end', '0.10017 for', '0.03880 link', '0.12690 from', '0.07611 advance', '0.06778 range', '0.08840 other', '0.11877 back', '0.10250 2', '0.11862 then', '0.04803 protect', '0.10844 when', '0.12027 while', '0.10857 5', '0.06515 went', '0.09557 between', '0.05951 attempt', '0.05864 higher', '0.10366 the', '0.08483 or', '0.11298 two', '0.09918 outside', '0.11811 on', '0.09331 them', '0.07741 roughly', '0.09232 by', '0.11102 with', '0.09843 around', '0.08435 start', '0.10193 each', '0.05510 thanks']\n",
      "-0.0495739\n",
      "['-0.03040 adapter', '-0.04594 nintendo', '-0.07493 japan', '-0.10813 ds', '-0.10630 device', '-0.04823 console', '-0.02858 mp3', '-0.02178 gameboy', '-0.02477 handhelds', '-0.03675 yen', '-0.05025 japanese', '-0.10402 gaming', '-0.04624 gadget', '-0.02790 handheld', '0.02452 psp', '-0.06348 sony']\n",
      "-0.0262723\n",
      "['-0.12171 move', '-0.02903 release', '-0.02770 packaged', '0.02919 selling', '-0.09871 add', '0.03158 priced', '0.04603 sold', '-0.05866 appeal', '0.01382 sale', '-0.07529 built', '-0.02501 releasing', '-0.00252 launch', '0.05135 sell', '-0.04037 market', '-0.04480 broaden', '-0.08598 plans', '-0.00898 unveiling', '-0.01550 price', '-0.03688 available']\n",
      "********************\n",
      "7\n",
      "0.0310315\n",
      "['-0.03822 work', '-0.03055 seen', '0.00532 link', '0.01608 add', '0.11564 will', '0.12972 means', '0.01001 difference', '-0.03605 no', '-0.01102 cost', '0.05373 lets', '0.00528 expected', '-0.04495 people', '0.11107 adds', '0.01062 purpose', '0.05266 dominates', '0.10486 goes', '0.00560 is', '0.09877 can']\n",
      "0.0901379\n",
      "['0.06213 well', '0.10737 widely', '0.09315 thought', '0.00034 world', '0.06014 as', '0.08165 also', '0.16467 has', '0.14360 be', '0.09763 it', '0.04617 successor', '0.02079 when', '0.06357 response', '0.11685 have', '0.02587 an', '0.15660 was', '0.10621 so', '0.07164 said', '0.04118 by', '0.10224 hugely', '0.15023 had', '0.17130 been', '0.09971 that']\n",
      "0.236192\n",
      "['0.32775 nintendo', '0.35520 japan', '0.33531 december', '0.35470 ds', '0.09416 mp3', '0.14317 gameboy', '0.05970 yen', '0.32865 japanese', '0.25407 sp', '0.23957 psp', '0.17810 tv', '0.32568 sony', '-0.00475 thanks', '0.31539 february']\n",
      "0.0861746\n",
      "['0.11640 player', '0.04486 move', '0.03165 against', '0.02303 successive', '0.02509 end', '0.20245 play', '0.12981 games', '0.09456 dominance', '0.20912 game', '0.07017 back', '-0.03049 then', '0.04169 went', '0.03480 attempt', '0.18307 playing', '0.11643 start']\n",
      "0.0680333\n",
      "['-0.00003 which', '0.04043 short', '0.10372 in', '0.09011 than', '0.03881 for', '0.11596 from', '0.01285 advance', '0.03673 range', '0.03024 million', '0.10711 other', '0.08718 2', '0.06298 more', '-0.00656 protect', '0.06461 while', '0.11184 5', '0.10421 between', '0.06399 higher', '0.02173 the', '0.06451 or', '0.10327 two', '0.07791 outside', '0.07015 on', '0.01375 them', '0.15785 roughly', '0.03862 with', '0.09678 around', '0.04734 one', '0.15651 each', '0.06034 about']\n",
      "0.0653494\n",
      "['0.02307 adapter', '0.14905 device', '0.13840 wireless', '-0.00304 touch', '-0.00314 console', '0.03349 versions', '0.10579 movies', '0.06241 handhelds', '0.03283 gaming', '0.07265 gadget', '0.08315 clips', '0.10484 handheld', '0.15000 screens', '-0.02214 dual', '0.00731 sensitive', '0.03168 media', '0.14094 screen', '0.05550 music', '0.00341 download', '0.06169 files', '0.10222 film', '0.10757 video']\n",
      "0.0607206\n",
      "['-0.03938 pre', '0.03618 release', '0.05803 packaged', '-0.00573 its', '0.16899 selling', '0.10780 priced', '0.17705 sold', '0.00166 successful', '0.01725 appeal', '0.15706 sale', '-0.02456 board', '0.00020 preparing', '0.02099 built', '0.07903 releasing', '0.13829 launch', '0.18481 sell', '0.02422 giant', '0.08004 market', '0.00856 broaden', '0.01788 plans', '0.12825 unveiling', '0.09304 price', '0.01074 multi', '0.01689 available']\n",
      "********************\n",
      "7\n",
      "-0.0648835\n",
      "['-0.15914 which', '0.02234 short', '-0.00188 player', '-0.07367 move', '-0.02038 pre', '0.05994 in', '-0.39182 device', '-0.27997 wireless', '0.05121 against', '0.02599 touch', '0.04687 work', '-0.05307 successive', '-0.20263 well', '-0.04165 end', '0.00577 than', '0.04538 thought', '-0.28013 release', '0.02821 play', '-0.02140 seen', '-0.26320 console', '-0.20788 packaged', '-0.17642 world', '-0.11888 its', '-0.00741 for', '-0.06709 link', '0.08127 from', '-0.14168 versions', '-0.10501 movies', '-0.25550 handhelds', '-0.00771 games', '0.01131 advance', '-0.05474 add', '-0.30605 yen', '-0.23555 as', '-0.08274 range', '-0.03011 dominance', '-0.11055 million', '0.03943 will', '-0.10860 gaming', '0.01588 game', '-0.01262 means', '0.04148 other', '0.09155 back', '-0.23605 gadget', '-0.07083 clips', '-0.22515 also', '-0.18590 has', '-0.14052 be', '0.04723 difference', '0.02328 it', '0.05387 no', '-0.09209 successful', '-0.31595 handheld', '-0.06666 appeal', '-0.01653 screens', '-0.15336 dual', '-0.10767 sensitive', '-0.03157 board', '-0.03091 2', '-0.11703 preparing', '-0.15417 built', '-0.29253 releasing', '-0.22078 successor', '-0.08188 media', '-0.09362 screen', '-0.20523 cost', '-0.10513 lets', '0.06613 more', '0.04586 then', '0.00851 protect', '-0.11218 music', '0.00450 when', '0.05100 while', '-0.20081 download', '-0.02869 5', '-0.09952 response', '-0.01423 have', '-0.13207 files', '-0.09209 expected', '0.03718 people', '0.05278 went', '0.03327 an', '0.05187 between', '-0.04986 attempt', '-0.11027 giant', '-0.05660 higher', '0.02266 the', '-0.05118 or', '-0.30443 market', '-0.07499 was', '-0.01304 adds', '0.06666 two', '0.04020 so', '0.06913 outside', '-0.03334 film', '-0.01020 purpose', '0.04040 said', '-0.02489 dominates', '0.01801 playing', '-0.12765 broaden', '0.02843 goes', '0.04703 on', '0.06306 them', '-0.08431 is', '-0.00703 roughly', '-0.15171 by', '-0.15727 plans', '0.05334 with', '0.04868 around', '-0.11857 start', '0.01128 one', '-0.17563 hugely', '0.07701 each', '-0.02961 thanks', '-0.00358 had', '-0.10974 been', '0.03647 can', '-0.03873 that', '0.04076 about', '-0.13876 multi', '-0.10906 video', '-0.19992 available']\n",
      "0.0\n",
      "['0.00000 adapter']\n",
      "0.410701\n",
      "['0.40392 japan', '0.41748 japanese']\n",
      "0.446799\n",
      "['0.42683 launch', '0.46677 unveiling']\n",
      "0.0114889\n",
      "['0.04215 nintendo', '0.00511 december', '-0.10333 ds', '0.08441 mp3', '0.07505 gameboy', '-0.04921 sp', '0.00997 psp', '0.06934 tv', '-0.03115 sony', '0.01256 february']\n",
      "0.406078\n",
      "['0.47143 selling', '0.32266 priced', '0.48009 sold', '0.40359 sale', '0.45786 sell', '0.30084 price']\n",
      "0.0\n",
      "['0.00000 widely']\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "true_text = BBCNews.data[-3]\n",
    "#print(true_text)\n",
    "#true_text = \" \".join(stopwords.words(\"english\")+x)\n",
    "text = true_text\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space    \n",
    "text = text.translate(translator).lower().split()\n",
    "true_text = true_text.translate(translator).lower().split()\n",
    "true_text = set([t for t in true_text if t in word2vec ])\n",
    "start = time.time()\n",
    "text = [t for t in text if t in word2vec ]\n",
    "text = np.array(list(set(text)))\n",
    "print(len(text))\n",
    "num_clusters = math.ceil(np.sqrt(len(set(text))))\n",
    "vecs = np.array([word2vec[w] for w in text])\n",
    "for model in [\"kmeans\", \"spectral\", \"ggm\"]:\n",
    "    similarity_matrix = cosine_similarity(vecs)+1\n",
    "    for i in range(7, 8):\n",
    "        if model == \"kmeans\":\n",
    "            sc = KMeans(n_clusters = i, precompute_distances = True)\n",
    "            sc.fit(vecs)\n",
    "            distance_matrix = euclidean_distances(vecs)\n",
    "        elif model == \"spectral\":\n",
    "            sc = SpectralClustering(i, affinity='precomputed', assign_labels = \"discretize\", n_init=100)     \n",
    "            sc.fit(similarity_matrix)\n",
    "            distance_matrix = 2-similarity_matrix\n",
    "        elif model == \"ggm\":\n",
    "            dpgmm = mixture.BayesianGaussianMixture(n_components=i, covariance_type='full').fit(vecs)\n",
    "            sc.labels_ = dpgmm.predict(vecs)\n",
    "\n",
    "        print(len(set(sc.labels_)))\n",
    "        silhoutte_scores = silhouette_samples(distance_matrix, sc.labels_, metric = 'precomputed')\n",
    "        non_empty_clusters = np.array(list(set(sc.labels_)))\n",
    "        indices = [np.where(sc.labels_ == j) if j in non_empty_clusters \n",
    "                                                           else [] for j in range(i)]\n",
    "        sh_means = np.array([np.mean(silhoutte_scores[indices[j]]) for j in non_empty_clusters])\n",
    "        for i in range(len(non_empty_clusters)):\n",
    "            if True:\n",
    "                sil_scores = silhoutte_scores[indices[i]]\n",
    "                #print(within_var(vecs[indices[i]]))\n",
    "                print(sh_means[i])\n",
    "                words = text[indices[i]].tolist()\n",
    "                output = ['{:.5f}'.format(s)+\" \"+w for s, w in zip(sil_scores, words)]\n",
    "                #output = [w for w in words]\n",
    "            print(output)\n",
    "    print(\"*\"*20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic soft cosine distance\n",
    "build_vec() transforms a text into a vector.\n",
    "Cluster_TSCD() computes the soft cosine similarity between two text vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-d59d1cda6715>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcluster_TSCD_auto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_vec_auto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_vec_auto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vec_auto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spectral\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "#similarity_matrix = np.ones((len(allTopics), len(allTopics)), np.float64)\n",
    "#for i in range(1, len(allTopics)):\n",
    "#    for j in range(i):\n",
    "#        x = time.time()\n",
    "#        distance_matrix[i, j] = euclidean(allTopics[i], allTopics[j])\n",
    "#        similarity_matrix[i, j] = 1-cosine(allTopics[i], allTopics[j])\n",
    "#        similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "class PuncSplitTokenizer():\n",
    "    \"\"\"\n",
    "        Map punctuations to space and then split, optional to remove predefined stopwords\n",
    "    \"\"\"\n",
    "    def __init__(self, stopwords):\n",
    "        self.translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "        self.stopwords = stopwords\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "        self.stopwords = []\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        tokens = text.translate(self.translator).lower().split()\n",
    "        if self.stopwords:\n",
    "            tokens = [t for t in tokens if t not in self.stopwords]\n",
    "        return tokens\n",
    "\n",
    "class TopicClusterGadget():\n",
    "    \"\"\"\n",
    "        For vectorizing and computing document distance, based on word-topic-clusters in a document. \n",
    "        Implemented wmd_cosine, wmd_euclidean, and soft_cosine\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, cls, distance_measure, word2vec):\n",
    "        self.cls = cls\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec = word2vec\n",
    "        self.distance_measure = distance_measure \n",
    "        if distance_measure == \"wmd_euclidean\":\n",
    "            self.get_distance = DistanceTool.wmd_distance_euclidean(vec1, vec2)\n",
    "        elif distance_measure == \"wmd_cosine\":\n",
    "            self.get_distance = DistanceTool.wmd_distance_cosine(vec1, vec2)\n",
    "        elif self.distance_measure == \"soft_cosine\":\n",
    "            self.get_distance = DistanceTool.softcosine_distance(vec1, vec2)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.tokenize(text)\n",
    "    \n",
    "    def vectorize_all(self, train_X, test_X):\n",
    "        train_vecs = [self.encode(t) for t in train_X]\n",
    "        test_vecs = [self.encode(t) for t in test_X]\n",
    "        max_len = np.max(np.concatenate(([len(a) for a in train_vecs], [len(a) for a in test_vecs])))\n",
    "        train_vecs = np.asarray([np.pad(a, (0, max_len - len(a)), 'constant', constant_values=0) for a in train_vecs])\n",
    "        test_vecs = np.asarray([np.pad(a, (0, max_len - len(a)), 'constant', constant_values=0) for a in test_vecs])\n",
    "        return train_vecs, test_vecs\n",
    "        \n",
    "    def encode(self, text):\n",
    "        text = self.tokenize(text)\n",
    "        text = [t for t in text if t in self.word2vec]\n",
    "        word_counter = Counter(text)\n",
    "        word_counts = np.array(list(word_counter.values()))\n",
    "        unique_text = list(word_counter.keys())\n",
    "        vecs = [self.word2vec[w] for w in unique_text]\n",
    "        num_clusters = math.ceil(np.sqrt(len(unique_text)))\n",
    "                                \n",
    "        if self.cls == \"kmeans\": \n",
    "            model = KMeans(n_clusters = num_clusters, random_state=3425, precompute_distances = True)\n",
    "            model.fit(vecs)\n",
    "            distance_matrix = euclidean_distances(vecs)\n",
    "            cluster_centers = model.cluster_centers_\n",
    "        elif self.cls == \"spectral\":\n",
    "            model = SpectralClustering(num_clusters, affinity='precomputed', random_state=3425,\n",
    "                                    assign_labels = \"discretize\", n_init=100)\n",
    "            similarity_matrix = cosine_similarity(vecs)+1 \n",
    "            model.fit(similarity_matrix) \n",
    "            distance_matrix = 2-similarity_matrix\n",
    "        labels = model.labels_\n",
    "        non_empty_clusters = np.array(list(set(labels)))                         \n",
    "        indices = [np.where(labels == i) if i in non_empty_clusters \n",
    "                                                           else [] for i in range(num_clusters)]\n",
    "        \n",
    "        #silhoutte_scores = silhouette_samples(distance_matrix, labels, metric = 'precomputed')\n",
    "        \n",
    "        sizes = np.array([sum(word_counts[indices[i]]) for i in non_empty_clusters])\n",
    "        weights = sizes/sum(sizes)                         \n",
    "        return np.concatenate([[num_clusters], cluster_centers.flatten(), weights])\n",
    "    \n",
    "    def decode(self, a, b):\n",
    "        num_clusters_a, num_clusters_b = int(a[0]), int(b[0])\n",
    "        cut_a, cut_b = 1+300*num_clusters_a, 1+300*num_clusters_b\n",
    "        topics1, topics2 = a[1:cut_a].reshape(-1,300), b[1:cut_b].reshape(-1,300)\n",
    "        weights1, weights2 = a[cut_a:cut_a+num_clusters_a], b[cut_b:cut_b+num_clusters_b]\n",
    "        allTopics = np.concatenate([topics1,topics2])\n",
    "        d1 = np.concatenate([weights1,[0]*len(topics2)])\n",
    "        d2 = np.concatenate([[0]*len(topics1),weights2])\n",
    "        return d1, d2, allTopics\n",
    "\n",
    "class DistanceTool():\n",
    "    \"\"\"\n",
    "        Implemented wmd_cosine, wmd_euclidean, and soft_cosine\n",
    "    \"\"\"\n",
    "    def softcosine_distance(self, vec1, vec2):\n",
    "        d1, d2, allTopics = self.decode(vec1, vec2)\n",
    "        similarity_matrix = cosine_similarity(allTopics)+1\n",
    "        result = d1.T.dot(similarity_matrix).dot(d2)\n",
    "        vec1 = d1.T.dot(similarity_matrix).dot(d1)\n",
    "        vec2 = d2.T.dot(similarity_matrix).dot(d2)\n",
    "        result /= (np.sqrt(vec1)*np.sqrt(vec2))\n",
    "        return 1-result\n",
    "    \n",
    "    def wmd_distance_euclidean(self, vec1, vec2):\n",
    "        return self.wmd_distance(vec1, vec2, euclidean_distances)\n",
    "    \n",
    "    def wmd_distance_cosine(self, vec1, vec2):\n",
    "        return self.wmd_distance(vec1, vec2, cosine_distances)\n",
    "    \n",
    "    def wmd_distance(self, vec1, vec2, distance_func):\n",
    "        d1, d2, allTopics = self.decode(vec1, vec2)\n",
    "        distance_matrix = distance_func(allTopics, allTopics)\n",
    "        return emd(d1, d2, distance_matrix)\n",
    "\n",
    "class WordAverageGadget():\n",
    "    \"\"\"\n",
    "        Averaging word vectors method\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cls = \"average\"\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.tokenize(text)\n",
    "    \n",
    "    def vectorize_all(self, train_X, test_X):\n",
    "        train_X = [self.encode(t) for t in train_X]\n",
    "        test_X = [self.encode(t) for t in test_X]\n",
    "        return train_X, test_X\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = self.tokenize(text)\n",
    "        return np.mean([self.word2vec[w] for w in text if w in self.word2vec], axis=0)\n",
    "        \n",
    "class WordMoversDistanceGadget():\n",
    "    \"\"\"\n",
    "        The original word mover's distance\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cls = \"average\"\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.tokenize(text)\n",
    "    \n",
    "    def vectorize_all(self, train_X, test_X):\n",
    "        train_X = [self.encode(t) for t in train_X]\n",
    "        test_X = [self.encode(t) for t in test_X]\n",
    "        return train_X, test_X\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = self.tokenize(text)\n",
    "        return np.mean([self.word2vec[w] for w in text if w in self.word2vec], axis=0)\n",
    "    \n",
    "        \n",
    "def euclidean(a, b):\n",
    "    return np.sqrt(np.sum(np.square(a-b)))\n",
    "\n",
    "def build_vec_auto(text, cls, strategy):\n",
    "    \"\"\"build vector represenetation of text\n",
    "        cls: Clustering model\n",
    "        strategy: used with spectral clustering \n",
    "            0: non-empy clusters, positive silhoutte scores, weighted average\n",
    "            1: \n",
    "    \"\"\"\n",
    "    x = text\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space    \n",
    "    text = text.translate(translator).lower().split()\n",
    "    start = time.time()\n",
    "    text = [t for t in text if t in word2vec and not t in stopwords.words(\"english\")]\n",
    "    if cls == \"kmeans\":\n",
    "        num_clusters = math.ceil(np.sqrt(len(set(text))))\n",
    "        vecs = [word2vec[w] for w in text]\n",
    "        km = KMeans(n_clusters = num_clusters, random_state=3425, precompute_distances = True)\n",
    "        km.fit(vecs)\n",
    "        non_empty_clusters = np.array(list(set(km.labels_)))\n",
    "        indices = [np.where(sc.labels_ == i) if i in non_empty_clusters \n",
    "                                                               else [] for i in range(num_clusters)]\n",
    "        \n",
    "        sizes = np.array([sum(word_counts[indices[i]]) for i in non_empty_clusters])\n",
    "        sizes_weights = sizes/sum(sizes)\n",
    "        cluster_sizes = np.array([len(np.where(km.labels_ == i)[0]) for i in range(num_clusters)])\n",
    "        weights = cluster_sizes/sum(cluster_sizes)\n",
    "        \n",
    "        return np.concatenate([[num_clusters], km.cluster_centers_.flatten(), weights])\n",
    "    elif cls == \"spectral\":\n",
    "        if strategy == 0:\n",
    "            word_counter = Counter(text)\n",
    "            unique_text = list(word_counter.keys())\n",
    "            word_counts = np.array(list(word_counter.values()))\n",
    "            if len(unique_text) < 3:\n",
    "                unique_text = unique_text*2\n",
    "                word_counts = word_counts*2\n",
    "            num_clusters = math.ceil(np.sqrt(len(unique_text)))\n",
    "            vecs = np.array([word2vec[w] for w in unique_text])\n",
    "            sc = SpectralClustering(num_clusters, affinity='precomputed', random_state=3425,\n",
    "                                    assign_labels = \"discretize\", n_init=100)\n",
    "            similarity_matrix = cosine_similarity(vecs)+1\n",
    "            sc.fit(similarity_matrix)\n",
    "            silhoutte_scores = silhouette_samples(2-similarity_matrix, sc.labels_, metric = 'precomputed')\n",
    "            non_empty_clusters = np.array(list(set(sc.labels_)))\n",
    "            indices = [np.where(sc.labels_ == i) if i in non_empty_clusters \n",
    "                                                               else [] for i in range(num_clusters)]\n",
    "            sh_means = np.array([np.mean(silhoutte_scores[indices[i]]) for i in non_empty_clusters])\n",
    "            pos_sh_means_indices = np.where(sh_means > 0)\n",
    "            sh_means = sh_means[pos_sh_means_indices]\n",
    "            good_clusters = non_empty_clusters[pos_sh_means_indices]\n",
    "            sh_weights = np.array(sh_means/sum(sh_means))\n",
    "            \n",
    "            sizes = np.array([sum(word_counts[indices[i]]) for i in good_clusters])\n",
    "            sizes_weights = sizes/sum(sizes)\n",
    "            p = 0.3\n",
    "            weights = p*sh_weights+(1-p)*sizes_weights\n",
    "            #print(\"{},{},{}\".format(sum(sh_weights), sum(sizes_weights), sum(weights)))\n",
    "            centroids = np.array([np.mean(vecs[indices[i]], axis = 0) for i in good_clusters])\n",
    "            rep = np.concatenate(([len(good_clusters)], centroids.flatten(), sizes_weights))\n",
    "            return rep\n",
    "        elif strategy == 1:\n",
    "            word_counter = Counter(text)\n",
    "            unique_text = list(word_counter.keys())\n",
    "            word_counts = list(word_counter.values())\n",
    "            if len(unique_text) < 3:\n",
    "                unique_text = unique_text*2\n",
    "                word_counts = word_counts*2\n",
    "            num_clusters = math.ceil(np.sqrt(len(unique_text)))\n",
    "            vecs = [word2vec[w] for w in unique_text]\n",
    "            sc = SpectralClustering(num_clusters, affinity='precomputed', random_state=3425,\n",
    "                                    assign_labels = \"discretize\", n_init=100)\n",
    "            similarity_matrix = cosine_similarity(vecs)+1\n",
    "            sc.fit(similarity_matrix)\n",
    "            silhoutte_scores = silhouette_samples(2-similarity_matrix, sc.labels_, metric = 'precomputed')\n",
    "            silhoutte_contribution = np.multiply(word_counts, silhoutte_scores)\n",
    "            total_contribution = sum(silhoutte_contribution)\n",
    "            non_empty_clusters = list(set(sc.labels_))\n",
    "            indices = [np.where(sc.labels_ == i)[0].tolist() if i in non_empty_clusters \n",
    "                                                               else [] for i in range(num_clusters)]\n",
    "            \n",
    "            weights = np.array([sum(np.take(silhoutte_contribution, indices[i]))/total_contribution\n",
    "                                                                        for i in non_empty_clusters])\n",
    "            #print(\"{},{},{}\".format(sum(sh_weights), sum(sizes_weights), sum(final_weights)))\n",
    "            centroids = np.array([np.mean(np.take(vecs, indices[i], axis = 0), axis = 0) for i in non_empty_clusters])\n",
    "            rep = np.concatenate([[len(non_empty_clusters)], centroids.flatten(), weights])\n",
    "            #for i in range(len(weights)):\n",
    "             #   print(weights[i])\n",
    "              #  print(np.take(unique_text, indices[non_empty_clusters[i]]))\n",
    "            #print(\"-----\")\n",
    "            return rep\n",
    "        elif strategy == 2:\n",
    "            word_counter = Counter(text)\n",
    "            unique_text = list(word_counter.keys())\n",
    "            word_counts = list(word_counter.values())\n",
    "            if len(unique_text) < 3:\n",
    "                unique_text = unique_text*2\n",
    "                word_counts = word_counts*2\n",
    "            num_clusters = 10#math.ceil(np.sqrt(len(unique_text)))\n",
    "            vecs = [word2vec[w] for w in unique_text]\n",
    "            sc = SpectralClustering(num_clusters, affinity='precomputed', random_state=3425,\n",
    "                                    assign_labels = \"discretize\", n_init=100)\n",
    "            similarity_matrix = cosine_similarity(vecs)+1\n",
    "            sc.fit(similarity_matrix)\n",
    "            non_empty_clusters = list(set(sc.labels_))\n",
    "            indices = [np.where(sc.labels_ == i)[0].tolist() if i in non_empty_clusters \n",
    "                                                               else [] for i in range(num_clusters)]\n",
    "            sizes = np.array([sum(np.take(word_counts, indices[i])) for i in non_empty_clusters])\n",
    "            centroids = np.array([np.mean(np.take(vecs, indices[i], axis = 0), axis = 0) for i in non_empty_clusters])\n",
    "            rep = np.concatenate([[len(non_empty_clusters)], centroids.flatten(), [1/len(sizes)]*len(sizes)])\n",
    "            return rep\n",
    "\n",
    "def cluster_TSCD_auto(a, b):\n",
    "    def euclidean(a, b):\n",
    "        return np.sqrt(np.sum(np.square(a-b)))\n",
    "    start = time.time()\n",
    "    num_clusters_a, num_clusters_b = int(a[0]), int(b[0])\n",
    "    cut_a, cut_b = 1+300*num_clusters_a, 1+300*num_clusters_b\n",
    "    topics1, topics2 = a[1:cut_a].reshape(-1,300), b[1:cut_b].reshape(-1,300)\n",
    "    weights1, weights2 = a[cut_a:cut_a+num_clusters_a], b[cut_b:cut_b+num_clusters_b]\n",
    "    allTopics = np.concatenate([topics1,topics2])\n",
    "    d1 = np.concatenate([weights1,[0]*len(topics2)])\n",
    "    d2 = np.concatenate([[0]*len(topics1),weights2])\n",
    "    similarity_matrix = cosine_similarity(allTopics)\n",
    "    result = d1.T.dot(similarity_matrix).dot(d2)\n",
    "    vec1 = d1.T.dot(similarity_matrix).dot(d1)\n",
    "    vec2 = d2.T.dot(similarity_matrix).dot(d2)\n",
    "    result /= (np.sqrt(vec1)*np.sqrt(vec2))\n",
    "    #print(time.time()-start)\n",
    "    return np.clip(1-result, 0 , 1)\n",
    "\n",
    "def cluster_TSCD(a, b, num_clusters):\n",
    "    def euclidean(a, b):\n",
    "        return np.sqrt(np.sum(np.square(a-b)))\n",
    "    cut = 300*num_clusters\n",
    "    topics1, topics2 = a[0:cut].reshape(-1,300), b[0:cut].reshape(-1,300)\n",
    "    freq1, freq2 = a[cut:], b[cut:]\n",
    "    allTopics = np.concatenate([topics1,topics2])\n",
    "    \n",
    "    d1 = np.concatenate([freq1/sum(freq1),[0]*len(topics2)])\n",
    "    d2 = np.concatenate([[0]*len(topics1),freq2/sum(freq2)])\n",
    "    similarity_matrix = np.ones((len(allTopics), len(allTopics)), np.float64)\n",
    "    for i in range(1, len(allTopics)):\n",
    "        for j in range(i):\n",
    "            #distance_matrix[i, j] = euclidean(allTopics[i], allTopics[j])\n",
    "            similarity_matrix[i, j] = 1-cosine(allTopics[i], allTopics[j])\n",
    "            similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "    result = d1.T.dot(similarity_matrix).dot(d2)\n",
    "    vec1 = d1.T.dot(similarity_matrix).dot(d1)\n",
    "    vec2 = d2.T.dot(similarity_matrix).dot(d2)\n",
    "    result /= (np.sqrt(vec1)*np.sqrt(vec2))\n",
    "    return np.clip(1-result, 0, 1)\n",
    "\n",
    "def cluster_TSCD_raw(text1, text2, num_clusters):\n",
    "    return cluster_TSCD(build_vec(text1, num_clusters), build_vec(text2, num_clusters), num_clusters)\n",
    "\n",
    "def cluster_TSCD_raw_auto(text1, text2, cls, strategy):\n",
    "    return cluster_TSCD_auto(build_vec_auto(text1, cls, strategy), build_vec_auto(text2, cls, strategy))\n",
    "\n",
    "x = build_vec_auto(texts[-1], \"spectral\", 0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word average distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wad(text):\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "    text = text.translate(translator).lower().split()\n",
    "    #text = [t for t in text if not t in stopwords.words(\"english\")]\n",
    "    return np.mean([word2vec[w] for w in text if w in word2vec], axis=0)\n",
    "\n",
    "def wordcount(text):\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "    text = text.translate(translator).lower().split()\n",
    "    text = [t for t in text if t in word2vec]\n",
    "    return len(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydata = BBCNews_noised\n",
    "train_X, test_X, train_y, test_y = train_test_split(mydata.data, mydata.label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started KNN \n",
      "287.6730680465698\n",
      "Vectorization finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"started KNN \")\n",
    "start = time.time()\n",
    "algo = \"TSCD\"\n",
    "if algo == \"wad\":\n",
    "    train_vecs = [ wad(t) for t in train_X]\n",
    "    test_vecs = [ wad(t) for t in test_X]\n",
    "elif algo == \"LSA\":\n",
    "    LsaVectorizer = make_pipeline(TfidfVectorizer(max_features = 20000, use_idf = True, min_df=2)\n",
    "                              ,TruncatedSVD(300))\n",
    "    train_vecs = LsaVectorizer.fit_transform(train_X)\n",
    "    test_vecs = LsaVectorizer.transform(test_X)\n",
    "elif algo == \"TSCD\":\n",
    "    train_vecs = [ build_vec_auto(t, 'spectral', 2) for t in train_X]\n",
    "    test_vecs = [ build_vec_auto(t, 'spectral', 2) for t in test_X]\n",
    "    max_len = np.max(np.concatenate(([len(a) for a in train_vecs], [len(a) for a in test_vecs])))\n",
    "    train_vecs = np.asarray([np.pad(a, (0, max_len - len(a)), 'constant', constant_values=0) for a in train_vecs])\n",
    "    test_vecs = np.asarray([np.pad(a, (0, max_len - len(a)), 'constant', constant_values=0) for a in test_vecs])\n",
    "print(time.time() - start)\n",
    "print(\"Vectorization finished\")\n",
    "#nbrs = GridSearchCV(KNeighborsClassifier(algorithm='ball_tree'), cv = None, \n",
    "                    #param_grid={\"n_neighbors\": range(5,20,5)})\n",
    "#nbrs = GridSearchCV(KNeighborsClassifier(algorithm='ball_tree', \n",
    " #                           metric=lambda a,b: cluster_TSCD_auto(a, b)), cv = None,\n",
    "  #                         param_grid={\"n_neighbors\": range(5,25,5)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354.6112232208252\n",
      "started predicting\n",
      "0.913173652695\n"
     ]
    }
   ],
   "source": [
    "if algo != \"TSCD\":\n",
    "    nbrs = GridSearchCV(KNeighborsClassifier(algorithm='ball_tree'), cv = None, \n",
    "                    param_grid={\"n_neighbors\": range(5,20,5)})\n",
    "else:\n",
    "    nbrs = KNeighborsClassifier(algorithm='ball_tree', n_neighbors = 10,\n",
    "                                metric=lambda a,b: cluster_TSCD_auto(a, b))\n",
    "nbrs.fit(train_vecs, train_y)\n",
    "print(time.time() - start)\n",
    "start = time.time()\n",
    "#test_vecs = [build_vec(t, num_clusters) for t in news20_test.data]\n",
    "\n",
    "print(\"started predicting\")\n",
    "start = time.time()\n",
    "predicted = nbrs.predict(test_vecs)\n",
    "print(accuracy_score(predicted, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94161676646706582]\n"
     ]
    }
   ],
   "source": [
    "datasets = [BBCNews]\n",
    "models = [#TopicClusterDistanceGadget(PuncSplitTokenizer(), \"kmeans\", \"soft_cosine\", word2vec),\n",
    "         WordAverageGadget(PuncSplitTokenizer(), word2vec)]\n",
    "\n",
    "accuracy = []\n",
    "for ds in datasets:\n",
    "    for model in models:\n",
    "        train_X, test_X, train_y, test_y = train_test_split(ds.data, ds.label, test_size=0.3, random_state=42)\n",
    "        train_vecs, test_vecs = model.vectorize_all(train_X, test_X)\n",
    "        if model.cls == \"average\":\n",
    "            nbrs = KNeighborsClassifier(algorithm='ball_tree', n_neighbors = 10)\n",
    "        else:\n",
    "            nbrs = KNeighborsClassifier(algorithm='ball_tree', n_neighbors = 10,\n",
    "                                metric=lambda a,b: model.get_distance(a, b))\n",
    "        nbrs.fit(train_vecs, train_y)\n",
    "        prediction = nbrs.predict(test_vecs)\n",
    "        accuracy.append(accuracy_score(prediction, test_y))\n",
    "        \n",
    "print(accuracy)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
