{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('../Benchmark/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyemd import emd \n",
    "import pickle\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gensim, math\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk \n",
    "from os.path import dirname, abspath\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import lda\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "import string\n",
    "import os\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.matutils import softcossim\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import mixture\n",
    "from collections import Counter\n",
    "import sklearn.metrics as mt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://ndownloader.figshare.com/files/5976069\n",
      "Downloading https://ndownloader.figshare.com/files/5976066\n",
      "Downloading https://ndownloader.figshare.com/files/5976063\n",
      "Downloading https://ndownloader.figshare.com/files/5976060\n",
      "Downloading https://ndownloader.figshare.com/files/5976057\n",
      "Downloading https://ndownloader.figshare.com/files/5976048\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-1c813b953ecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_rcv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrcv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_rcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrcv1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: (1,)"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_rcv1\n",
    "rcv1 = fetch_rcv1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "topics = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "cwd = dirname(os.getcwd())+\"/Data/bbc2/\"\n",
    "BBCNews = dotdict({'data': [],\n",
    "           'label': []})\n",
    "noise_indices= random.sample(range(0, len(noises)), 2225)\n",
    "add_noise = True \n",
    "counter = 0\n",
    "nn = 100\n",
    "for i, s in enumerate(topics):\n",
    "    path = cwd+s\n",
    "    for filename in os.listdir(path):\n",
    "        with open(path+\"/\"+filename, 'r', encoding=\"latin-1\") as f:\n",
    "            text = f.read() \n",
    "            text = text+(\" \"+noises[noise_indices[counter]]+\" \")*nn if add_noise else text\n",
    "            BBCNews.data.append(text)\n",
    "            BBCNews.label.append(i)\n",
    "        f.close()\n",
    "        counter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seeds_indices = random.sample(range(2225), 222)\n",
    "noises = [BBCNews.data[t] for t in seeds_indices]\n",
    "noises = \" \".join(noises)\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space    \n",
    "noises = noises.translate(translator).lower().split()\n",
    "noises = list(set(noises))\n",
    "noises = [t for t in noises if t in word2vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(texts, open(\"sample_texts.pickle\", \"wb\"))\n",
    "texts = pickle.load(open(\"sample_texts.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [2, 3, 4]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0,1,2,3,4],[2,3,4,5,6]])\n",
    "x[:,:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic soft cosine distance\n",
    "build_vec() transforms a text into a vector.\n",
    "Cluster_TSCD() computes the soft cosine similarity between two text vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12.        ,   0.03208542,   0.0886898 , ...,   0.14396887,\n",
       "         0.05836576,   0.06225681])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#similarity_matrix = np.ones((len(allTopics), len(allTopics)), np.float64)\n",
    "#for i in range(1, len(allTopics)):\n",
    "#    for j in range(i):\n",
    "#        x = time.time()\n",
    "#        distance_matrix[i, j] = euclidean(allTopics[i], allTopics[j])\n",
    "#        similarity_matrix[i, j] = 1-cosine(allTopics[i], allTopics[j])\n",
    "#        similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "\n",
    "def build_vec_auto(text, cls, strategy):\n",
    "    \"\"\"build vector represenetation of text\n",
    "        cls: Clustering model\n",
    "        strategy: used with spectral clustering \n",
    "            0: non-empy clusters, positive silhoutte scores, weighted average\n",
    "            1: \n",
    "    \"\"\"\n",
    "    x = text\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space    \n",
    "    text = text.translate(translator).lower().split()\n",
    "    start = time.time()\n",
    "    text = [t for t in text if t in word2vec and not t in stopwords.words(\"english\")]\n",
    "    if cls == \"kmeans\":\n",
    "        num_clusters = math.ceil(np.sqrt(len(set(text))))\n",
    "        vecs = [word2vec[w] for w in text]\n",
    "        km = KMeans(n_clusters = num_clusters, random_state=3425, precompute_distances = True)\n",
    "        km.fit(vecs)\n",
    "        cluster_sizes = np.array([len(np.where(km.labels_ == i)[0]) for i in range(num_clusters)])\n",
    "        weights = cluster_sizes/sum(cluster_sizes)\n",
    "        return np.concatenate([[num_clusters], km.cluster_centers_.flatten(), weights])\n",
    "    elif cls == \"spectral\":\n",
    "        if strategy == 0:\n",
    "            word_counter = Counter(text)\n",
    "            unique_text = list(word_counter.keys())\n",
    "            word_counts = np.array(list(word_counter.values()))\n",
    "            if len(unique_text) < 3:\n",
    "                unique_text = unique_text*2\n",
    "                word_counts = word_counts*2\n",
    "            num_clusters = math.ceil(np.sqrt(len(unique_text)))\n",
    "            vecs = np.array([word2vec[w] for w in unique_text])\n",
    "            sc = SpectralClustering(num_clusters, affinity='precomputed', random_state=3425,\n",
    "                                    assign_labels = \"discretize\", n_init=100)\n",
    "            similarity_matrix = cosine_similarity(vecs)+1\n",
    "            sc.fit(similarity_matrix)\n",
    "            silhoutte_scores = silhouette_samples(2-similarity_matrix, sc.labels_, metric = 'precomputed')\n",
    "            non_empty_clusters = np.array(list(set(sc.labels_)))\n",
    "            indices = [np.where(sc.labels_ == i) if i in non_empty_clusters \n",
    "                                                               else [] for i in range(num_clusters)]\n",
    "            sh_means = np.array([np.mean(silhoutte_scores[indices[i]]) for i in non_empty_clusters])\n",
    "            pos_sh_means_indices = np.where(sh_means > 0)\n",
    "            sh_means = sh_means[pos_sh_means_indices]\n",
    "            good_clusters = non_empty_clusters[pos_sh_means_indices]\n",
    "            sh_weights = np.array(sh_means/sum(sh_means))\n",
    "            \n",
    "            sizes = np.array([sum(word_counts[indices[i]]) for i in good_clusters])\n",
    "            sizes_weights = sizes/sum(sizes)\n",
    "            p = 0.3\n",
    "            weights = p*sh_weights+(1-p)*sizes_weights\n",
    "            #print(\"{},{},{}\".format(sum(sh_weights), sum(sizes_weights), sum(weights)))\n",
    "            centroids = np.array([np.mean(vecs[indices[i]], axis = 0) for i in good_clusters])\n",
    "            rep = np.concatenate(([len(good_clusters)], centroids.flatten(), sizes_weights))\n",
    "            return rep\n",
    "        elif strategy == 1:\n",
    "            word_counter = Counter(text)\n",
    "            unique_text = list(word_counter.keys())\n",
    "            word_counts = list(word_counter.values())\n",
    "            if len(unique_text) < 3:\n",
    "                unique_text = unique_text*2\n",
    "                word_counts = word_counts*2\n",
    "            num_clusters = math.ceil(np.sqrt(len(unique_text)))\n",
    "            vecs = [word2vec[w] for w in unique_text]\n",
    "            sc = SpectralClustering(num_clusters, affinity='precomputed', random_state=3425,\n",
    "                                    assign_labels = \"discretize\", n_init=100)\n",
    "            similarity_matrix = cosine_similarity(vecs)+1\n",
    "            sc.fit(similarity_matrix)\n",
    "            silhoutte_scores = silhouette_samples(2-similarity_matrix, sc.labels_, metric = 'precomputed')\n",
    "            silhoutte_contribution = np.multiply(word_counts, silhoutte_scores)\n",
    "            total_contribution = sum(silhoutte_contribution)\n",
    "            non_empty_clusters = list(set(sc.labels_))\n",
    "            indices = [np.where(sc.labels_ == i)[0].tolist() if i in non_empty_clusters \n",
    "                                                               else [] for i in range(num_clusters)]\n",
    "            \n",
    "            weights = np.array([sum(np.take(silhoutte_contribution, indices[i]))/total_contribution\n",
    "                                                                        for i in non_empty_clusters])\n",
    "            #print(\"{},{},{}\".format(sum(sh_weights), sum(sizes_weights), sum(final_weights)))\n",
    "            centroids = np.array([np.mean(np.take(vecs, indices[i], axis = 0), axis = 0) for i in non_empty_clusters])\n",
    "            rep = np.concatenate([[len(non_empty_clusters)], centroids.flatten(), weights])\n",
    "            #for i in range(len(weights)):\n",
    "             #   print(weights[i])\n",
    "              #  print(np.take(unique_text, indices[non_empty_clusters[i]]))\n",
    "            #print(\"-----\")\n",
    "            return rep\n",
    "        elif strategy == 2:\n",
    "            word_counter = Counter(text)\n",
    "            unique_text = list(word_counter.keys())\n",
    "            word_counts = list(word_counter.values())\n",
    "            if len(unique_text) < 3:\n",
    "                unique_text = unique_text*2\n",
    "                word_counts = word_counts*2\n",
    "            num_clusters = 10#math.ceil(np.sqrt(len(unique_text)))\n",
    "            vecs = [word2vec[w] for w in unique_text]\n",
    "            sc = SpectralClustering(num_clusters, affinity='precomputed', random_state=3425,\n",
    "                                    assign_labels = \"discretize\", n_init=100)\n",
    "            similarity_matrix = cosine_similarity(vecs)+1\n",
    "            sc.fit(similarity_matrix)\n",
    "            non_empty_clusters = list(set(sc.labels_))\n",
    "            indices = [np.where(sc.labels_ == i)[0].tolist() if i in non_empty_clusters \n",
    "                                                               else [] for i in range(num_clusters)]\n",
    "            sizes = np.array([sum(np.take(word_counts, indices[i])) for i in non_empty_clusters])\n",
    "            centroids = np.array([np.mean(np.take(vecs, indices[i], axis = 0), axis = 0) for i in non_empty_clusters])\n",
    "            rep = np.concatenate([[len(non_empty_clusters)], centroids.flatten(), [1/len(sizes)]*len(sizes)])\n",
    "            return rep\n",
    "\n",
    "def cluster_TSCD_auto(a, b):\n",
    "    def euclidean(a, b):\n",
    "        return np.sqrt(np.sum(np.square(a-b)))\n",
    "    start = time.time()\n",
    "    num_clusters_a, num_clusters_b = int(a[0]), int(b[0])\n",
    "    cut_a, cut_b = 1+300*num_clusters_a, 1+300*num_clusters_b\n",
    "    topics1, topics2 = a[1:cut_a].reshape(-1,300), b[1:cut_b].reshape(-1,300)\n",
    "    weights1, weights2 = a[cut_a:cut_a+num_clusters_a], b[cut_b:cut_b+num_clusters_b]\n",
    "    allTopics = np.concatenate([topics1,topics2])\n",
    "    d1 = np.concatenate([weights1,[0]*len(topics2)])\n",
    "    d2 = np.concatenate([[0]*len(topics1),weights2])\n",
    "    similarity_matrix = cosine_similarity(allTopics)\n",
    "    result = d1.T.dot(similarity_matrix).dot(d2)\n",
    "    vec1 = d1.T.dot(similarity_matrix).dot(d1)\n",
    "    vec2 = d2.T.dot(similarity_matrix).dot(d2)\n",
    "    result /= (np.sqrt(vec1)*np.sqrt(vec2))\n",
    "    #print(time.time()-start)\n",
    "    return np.clip(1-result, 0 , 1)\n",
    "\n",
    "def cluster_TSCD(a, b, num_clusters):\n",
    "    def euclidean(a, b):\n",
    "        return np.sqrt(np.sum(np.square(a-b)))\n",
    "    cut = 300*num_clusters\n",
    "    topics1, topics2 = a[0:cut].reshape(-1,300), b[0:cut].reshape(-1,300)\n",
    "    freq1, freq2 = a[cut:], b[cut:]\n",
    "    allTopics = np.concatenate([topics1,topics2])\n",
    "    \n",
    "    d1 = np.concatenate([freq1/sum(freq1),[0]*len(topics2)])\n",
    "    d2 = np.concatenate([[0]*len(topics1),freq2/sum(freq2)])\n",
    "    similarity_matrix = np.ones((len(allTopics), len(allTopics)), np.float64)\n",
    "    for i in range(1, len(allTopics)):\n",
    "        for j in range(i):\n",
    "            #distance_matrix[i, j] = euclidean(allTopics[i], allTopics[j])\n",
    "            similarity_matrix[i, j] = 1-cosine(allTopics[i], allTopics[j])\n",
    "            similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "    result = d1.T.dot(similarity_matrix).dot(d2)\n",
    "    vec1 = d1.T.dot(similarity_matrix).dot(d1)\n",
    "    vec2 = d2.T.dot(similarity_matrix).dot(d2)\n",
    "    result /= (np.sqrt(vec1)*np.sqrt(vec2))\n",
    "    return np.clip(1-result, 0, 1)\n",
    "\n",
    "def cluster_TSCD_raw(text1, text2, num_clusters):\n",
    "    return cluster_TSCD(build_vec(text1, num_clusters), build_vec(text2, num_clusters), num_clusters)\n",
    "\n",
    "def cluster_TSCD_raw_auto(text1, text2, cls, strategy):\n",
    "    return cluster_TSCD_auto(build_vec_auto(text1, cls, strategy), build_vec_auto(text2, cls, strategy))\n",
    "\n",
    "x = build_vec_auto(texts[-1], \"spectral\", 0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word average distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wad(text):\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "    text = text.translate(translator).lower().split()\n",
    "    #text = [t for t in text if not t in stopwords.words(\"english\")]\n",
    "    return np.mean([word2vec[w] for w in text if w in word2vec], axis=0)\n",
    "\n",
    "def wordcount(text):\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "    text = text.translate(translator).lower().split()\n",
    "    text = [t for t in text if t in word2vec]\n",
    "    return len(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started KNN \n",
      "2.479295015335083\n",
      "Vectorization finished\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(BBCNews.data, BBCNews.label, test_size=0.3, random_state=42)\n",
    "print(\"started KNN \")\n",
    "start = time.time()\n",
    "algo = \"wad\"\n",
    "if algo == \"wad\":\n",
    "    train_vecs = [ wad(t) for t in train_X]\n",
    "    test_vecs = [ wad(t) for t in test_X]\n",
    "elif algo == \"LSA\":\n",
    "    LsaVectorizer = make_pipeline(TfidfVectorizer(max_features = 20000, use_idf = True, min_df=2)\n",
    "                              ,TruncatedSVD(300))\n",
    "    train_vecs = LsaVectorizer.fit_transform(train_X)\n",
    "    test_vecs = LsaVectorizer.transform(test_X)\n",
    "elif algo == \"TSCD\":\n",
    "    train_vecs = [ build_vec_auto(t, 'spectral', 2) for t in train_X]\n",
    "    test_vecs = [ build_vec_auto(t, 'spectral', 2) for t in test_X]\n",
    "    max_len = np.max(np.concatenate(([len(a) for a in train_vecs], [len(a) for a in test_vecs])))\n",
    "    train_vecs = np.asarray([np.pad(a, (0, max_len - len(a)), 'constant', constant_values=0) for a in train_vecs])\n",
    "    test_vecs = np.asarray([np.pad(a, (0, max_len - len(a)), 'constant', constant_values=0) for a in test_vecs])\n",
    "print(time.time() - start)\n",
    "print(\"Vectorization finished\")\n",
    "#nbrs = GridSearchCV(KNeighborsClassifier(algorithm='ball_tree'), cv = None, \n",
    "                    #param_grid={\"n_neighbors\": range(5,20,5)})\n",
    "#nbrs = GridSearchCV(KNeighborsClassifier(algorithm='ball_tree', \n",
    " #                           metric=lambda a,b: cluster_TSCD_auto(a, b)), cv = None,\n",
    "  #                         param_grid={\"n_neighbors\": range(5,25,5)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.317711114883423\n",
      "started predicting\n",
      "0.940119760479\n"
     ]
    }
   ],
   "source": [
    "if algo != \"TSCD\":\n",
    "    nbrs = GridSearchCV(KNeighborsClassifier(algorithm='ball_tree'), cv = None, \n",
    "                    param_grid={\"n_neighbors\": range(5,20,5)})\n",
    "else:\n",
    "    nbrs = KNeighborsClassifier(algorithm='ball_tree', n_neighbors = 10,\n",
    "                                metric=lambda a,b: cluster_TSCD_auto(a, b))\n",
    "nbrs.fit(train_vecs, train_y)\n",
    "print(time.time() - start)\n",
    "start = time.time()\n",
    "#test_vecs = [build_vec(t, num_clusters) for t in news20_test.data]\n",
    "\n",
    "print(\"started predicting\")\n",
    "start = time.time()\n",
    "predicted = nbrs.predict(test_vecs)\n",
    "print(accuracy_score(predicted, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.97      0.93       153\n",
      "          1       0.93      0.96      0.95       114\n",
      "          2       0.97      0.85      0.91       136\n",
      "          3       0.99      0.99      0.99       141\n",
      "          4       0.94      0.94      0.94       124\n",
      "\n",
      "avg / total       0.94      0.94      0.94       668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mt.classification_report(predicted, test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
