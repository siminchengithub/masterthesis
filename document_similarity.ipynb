{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('../Benchmark/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyemd import emd  \n",
    "import gensim, math\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk \n",
    "from os.path import dirname, abspath\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import lda\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "import string\n",
    "import os\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.matutils import softcossim\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import mixture\n",
    "from collections import Counter\n",
    "import sklearn.metrics as mt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://ndownloader.figshare.com/files/5976069\n",
      "Downloading https://ndownloader.figshare.com/files/5976066\n",
      "Downloading https://ndownloader.figshare.com/files/5976063\n",
      "Downloading https://ndownloader.figshare.com/files/5976060\n",
      "Downloading https://ndownloader.figshare.com/files/5976057\n",
      "Downloading https://ndownloader.figshare.com/files/5976048\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-1c813b953ecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_rcv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrcv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_rcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrcv1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: (1,)"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_rcv1\n",
    "rcv1 = fetch_rcv1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "topics = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "cwd = dirname(os.getcwd())+\"/Data/bbc2/\"\n",
    "BBCNews = dotdict({'data': [],\n",
    "           'label': []})\n",
    "for i, s in enumerate(topics):\n",
    "    path = cwd+s\n",
    "    for filename in os.listdir(path):\n",
    "        with open(path+\"/\"+filename, 'r', encoding=\"latin-1\") as f:\n",
    "            text = f.read()\n",
    "            BBCNews.data.append(text)\n",
    "            BBCNews.label.append(i)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n",
      "2225\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "texts = [\"economics money finance policy politics economy\",\n",
    "         \"cat cow dog tiger lion elephant monkey\",\n",
    "         \"microeconomics consumption market stock currency production\",\n",
    "        \"Obama speaks to the media in Illinois\",\n",
    "         \"The president greets the press in Chicago\",\n",
    "        \"It's sunny outside, let's have lunch in the sun.\",\n",
    "        \"the weather is really nice, why don't we eat outside\",\n",
    "        \"snail tastes not aweful, french people are weird\",\n",
    "        \"It's sunny outside.\",\n",
    "        \"Let's have lunch in the sun.\",\n",
    "         \"Artificial intelligence (AI) makes it possible for machines to learn from experience, \\\n",
    "         adjust to new inputs and perform human-like tasks. Most AI examples that you hear about today – from \\\n",
    "         chess-playing computers to self-driving cars – rely heavily on deep learning and natural language \\\n",
    "         processing. Using these technologies, computers can be trained to accomplish specific tasks by processing \\\n",
    "         large amounts of data and recognizing patterns in the data.The term artificial intelligence was coined in \\\n",
    "         1956, but AI has become more popular today thanks to increased data volumes, advanced algorithms, and \\\n",
    "         improvements in computing power and storage.AI adds intelligence to existing products. In most cases, AI \\\n",
    "         will not be sold as an individual application. Rather, products you already use will be improved with AI \\\n",
    "         capabilities, much like Siri was added as a feature to a new generation of Apple products. Automation, \\\n",
    "         conversational platforms, bots and smart machines can be combined with large amounts of data to improve \\\n",
    "         many technologies at home and in the workplace, from security intelligence to investment analysis.\",\n",
    "         \"Artificial intelligence (AI, also machine intelligence, MI) is intelligence demonstrated by machines, \\\n",
    "in contrast to the natural intelligence (NI) displayed by humans and other animals. In computer science AI research \\\n",
    "is defined as the study of intelligent agents: any device that perceives its environment and takes actions that \\\n",
    "maximize its chance of successfully achieving its goals. Colloquially, the term artificial intelligence is applied \\\n",
    "when a machine mimics cognitive functions that humans associate with other human minds, such as learning and \\\n",
    "problem solving The scope of AI is disputed: as machines become increasingly capable, tasks considered as requiring \\\n",
    "intelligence are often removed from the definition, a phenomenon known as the AI effect, leading to the quip, \\\n",
    "AI is whatever hasn't been done yet. For instance, optical character recognition is frequently excluded from \\\n",
    "artificial intelligence, having become a routine technology. Capabilities generally classified as AI as of 2017 \\\n",
    "include successfully understanding human speech, competing at the highest level in strategic game systems (such as \\\n",
    "chess and Go), autonomous cars, intelligent routing in content delivery network and military simulations. \\\n",
    "Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several \\\n",
    "waves of optimism followed by disappointment and the loss of funding (known as an AI winter), followed by new \\\n",
    "approaches, success and renewed funding. For most of its history, AI research has been divided into subfields that \\\n",
    "often fail to communicate with each other. These sub-fields are based on technical considerations, such as particular \\\n",
    "goals (e.g. robotics or machine learning), the use of particular tools (logic or neural networks), or deep \\\n",
    "philosophical differences. Subfields have also been based on social factors (particular institutions or the work of \\\n",
    "particular researchers). The traditional problems (or goals) of AI research include reasoning, knowledge \\\n",
    "representation, planning, learning, natural language processing, perception and the ability to move and manipulate \\\n",
    "objects. General intelligence is among the field's long-term goals. Approaches include statistical methods, \\\n",
    "computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and \\\n",
    "mathematical optimization, neural networks and methods based on statistics, probability and economics. The AI field \\\n",
    "draws upon computer science, mathematics, psychology, linguistics, philosophy and many others.The field was founded \\\n",
    "on the claim that human intelligence can be so precisely described that a machine can be made to simulate it This \\\n",
    "raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with \\\n",
    "human-like intelligence which are issues that have been explored by myth, fiction and philosophy since antiquity. \\\n",
    "Some people also consider AI to be a danger to humanity if it progresses unabatedly\\\n",
    "Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment.\\\n",
    "In the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer \\\n",
    "power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the \\\n",
    "technology industry, helping to solve many challenging problems in computer science.\",\n",
    "        \"UK pioneers digital film network The world's first digital cinema network will be established in the UK over the next 18 months.\\\n",
    "        The UK Film Council has awarded a contract worth £11.5m to Arts Alliance Digital Cinema (AADC), who will set up the network of up to \\\n",
    "        250 screens. AADC will oversee the selection of cinemas across the UK which will use the digital equipment. High definition projectors \\\n",
    "        and computer servers will be installed to show mainly British and specialist films. Most cinemas currently have mechanical projectors but \\\n",
    "        the new network will see up to 250 screens in up to 150 cinemas fitted with digital projectors capable of displaying high definition images. \\\n",
    "        The new network will double the world's total of digital screens. Cinemas will be given the film on a portable hard drive and they will then \\\n",
    "        copy the content to a computer server.\\\n",
    "        Each film is about 100 gigabytes and has been compressed from an original one terabyte-size file. Fiona Deans, associate director of AADC, \\\n",
    "        said the compression was visually lossless so no picture degradation will occur.\\\n",
    "        The film will all be encrypted to prevent piracy and each cinema will have an individual key which will unlock the movie. People will see \\\n",
    "        the picture quality is a bit clearer with no scratches. The picture will look exactly the same as when the print was first made - there is no \\\n",
    "        degradation in quality over time. The key benefit of the digital network will be an increase in the distribution and screening of British films, \\\n",
    "        documentaries and foreign language films.Access to specialised film is currently restricted across the UK, said Pete Buckingham, head of Distribution \\\n",
    "        and Exhibition at the UK Film Council. Although a genuine variety of films is available in central London and a few other metropolitan areas, the choice \\\n",
    "        for many outside these areas remains limited, and the Digital Screen Network will improve access for audiences across the UK, Digital prints costs less than \\\n",
    "        a traditional 35mm print - giving distributors more flexibility in how they screen films, said Ms Deans. It can cost up to £1,500 to make a copy of a print for \\\n",
    "        specialist films. In the digital world you can make prints for considerably less than that. Distributors can then send out prints to more cinemas and prints can \\\n",
    "        stay in cinemas for much longer. The UK digital network will be the first to employ 2k projectors - which are capable of showing films at resolutions of 2048 * 1080 pixels. \\\n",
    "        A separate competitive process to determine which cinemas will receive the digital screening technology will conclude in May. The sheer cost of traditional prints means that \\\n",
    "        some cinemas need to show them twice a day in order to recoup costs. Some films need word of mouth and time to build momentum - they don't need to be shown twice a day, explained \\\n",
    "        Ms Deans. A cinema will often book a 35mm print in for two weeks - even if the film is a roaring success they cannot hold on to the print because it will have to go to another cinema. \\\n",
    "        With digital prints, every cinema will have its own copy.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001560211181640625\n",
      "0.556812047958374\n",
      "0.011298179626464844\n",
      "0.0280084\n",
      "['established' 'awarded' 'set' 'installed' 'fitted' 'given' 'made'\n",
      " 'restricted' 'available' 'giving' 'employ' 'receive' 'don']\n",
      "-0.0391861\n",
      "['pioneers' 'world' 's' 'alliance' 'selection' 'new' 'copy' 'content' 'an'\n",
      " 'original' 'piracy' 'key' 'quality' 'distribution' 'foreign' 'language'\n",
      " 'genuine' 'choice' 'distributors' 'flexibility' 'competitive' 'process'\n",
      " 'order' 'word' 'mouth' 'book' 'success' 'cannot' 'own']\n",
      "0.0141266\n",
      "['use' 'see' 'capable' 'drive' 'occur' 'prevent' 'unlock' 'look' 'benefit'\n",
      " 'access' 'improve' 'make' 'you' 'send' 'stay' 'determine' 'conclude'\n",
      " 'need' 'build' 'hold' 'go']\n",
      "0.0872931\n",
      "['mainly' 'most' 'all' 'individual' 'variety' 'few' 'other' 'many' 'these'\n",
      " 'limited' 'traditional' 'some' 'often']\n",
      "0.27641\n",
      "['film' 'cinema' 'cinemas' 'films' 'movie' 'screening' 'documentaries'\n",
      " 'audiences']\n",
      "0.0959104\n",
      "['digital' 'network' 'equipment' 'projectors' 'computer' 'servers'\n",
      " 'portable' 'server' 'gigabytes' 'terabyte' 'file' 'compression' 'lossless'\n",
      " 'encrypted' 'technology']\n",
      "0.020893\n",
      "['first' 'over' 'next' 'up' 'double' 'then' 'each' 'one' 'same' 'out'\n",
      " 'separate' 'twice' 'day' 'momentum' 'two' 'roaring' 'another' 'every']\n",
      "0.0236164\n",
      "['months' 'contract' 'worth' '5m' 'total' 'about' 'increase' 'costs' 'less'\n",
      " 'than' 'more' 'cost' 'considerably' 'recoup' 'weeks']\n",
      "0.0384027\n",
      "['the' 'in' 'across' 'which' 'high' 'currently' 'with' 'on' 'from' 'at'\n",
      " 'central' 'metropolitan' 'for' 'outside' 'its']\n",
      "0.0916785\n",
      "['who' 'have' 'but' 'hard' 'they' 'so' 'no' 'people' 'bit' 'exactly' 'as'\n",
      " 'when' 'there' 'time' 'although' 'how' 'it' 'that' 'much' 'longer' 'them'\n",
      " 'even' 'if' 'because']\n",
      "0.07086\n",
      "['council' 'arts' 'oversee' 'specialist' 'deans' 'associate' 'director'\n",
      " 'said' 'head' 'explained']\n",
      "-0.0152783\n",
      "['screens' 'definition' 'show' 'mechanical' 'displaying' 'images'\n",
      " 'compressed' 'size' 'visually' 'picture' 'degradation' 'clearer'\n",
      " 'scratches' 'print' 'exhibition' 'screen' 'prints' 'showing' 'resolutions'\n",
      " 'pixels' 'sheer' 'shown']\n",
      "0.10524\n",
      "['will' 'be' 'has' 'is' 'been' 'was' 'areas' 'remains' 'can' 'are' 'may'\n",
      " 'means']\n",
      "0.202026\n",
      "['uk' 'british' 'fiona' 'pete' 'london' 'ms' '2k' 't']\n"
     ]
    }
   ],
   "source": [
    "#gmm = mixture.BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', \n",
    "#                              init_params = 'kmeans', \n",
    "#                              weight_concentration_prior_type = 'dirichlet_process').fit(vecs)\n",
    "#labels = gmm.predict(vecs)\n",
    "\n",
    "#km = KMeans(n_clusters = num_clusters)\n",
    "#km.fit(vecs)\n",
    "start = time.time()\n",
    "text = texts[-1]\n",
    "x = texts[-1]\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space    \n",
    "text = text.translate(translator).lower().split()\n",
    "text = [t for t in text if t in word2vec ]\n",
    "wordSum = len(text)\n",
    "word_counter = Counter(text)\n",
    "unique_text = list(word_counter.keys())\n",
    "word_counts = list(word_counter.values())\n",
    "num_clusters = int(np.sqrt(len(unique_text)))#int(np.sqrt(len(unique_text)))\n",
    "#print(num_clusters)\n",
    "vecs = [word2vec[w] for w in unique_text]\n",
    "print(time.time() - start)\n",
    "start = time.time()\n",
    "sc = SpectralClustering(num_clusters, affinity='precomputed', \n",
    "                        assign_labels = \"kmeans\", n_init=100)\n",
    "similarity_matrix = cosine_similarity(vecs)\n",
    "sc.fit(similarity_matrix)\n",
    "print(time.time() - start)\n",
    "start = time.time()\n",
    "silhoutte_scores = silhouette_samples(1-similarity_matrix, sc.labels_, metric = 'precomputed')\n",
    "indices = [np.where(sc.labels_ == i)[0].tolist() for i in range(num_clusters)]\n",
    "\n",
    "sh_means = [np.mean(np.take(silhoutte_scores, indices[i])) for i in range(num_clusters)]\n",
    "sh_weights = np.array(sh_means/sum(sh_means))\n",
    "\n",
    "sizes_weights = np.array([sum(np.take(word_counts, indices[i]))/wordSum for i in range(num_clusters)])\n",
    "p = 0.3\n",
    "final_weights = p*sh_weights+(1-p)*sizes_weights\n",
    "#print(\"{},{},{}\".format(sum(sh_weights), sum(sizes_weights), sum(final_weights)))\n",
    "centroids = np.array([np.mean(np.take(vecs, indices[i], axis = 0), axis = 0) for i in range(num_clusters)])\n",
    "rep = np.concatenate([[num_clusters], centroids.flatten(), final_weights])\n",
    "print(time.time() - start)\n",
    "#print(\"{},{}\".format(len(text), len(vecs)))\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(sh_weights[i])\n",
    "    print(\"{}\".format(np.take(unique_text, np.where(sc.labels_ == i)[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,-1])\n",
    "x[x>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic soft cosine distance\n",
    "build_vec() transforms a text into a vector.\n",
    "Cluster_TSCD() computes the soft cosine similarity between two text vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.0\n",
      "time:0.20522212982177734\n",
      "score: 0.0\n",
      "time:0.360170841217041\n"
     ]
    }
   ],
   "source": [
    "#similarity_matrix = np.ones((len(allTopics), len(allTopics)), np.float64)\n",
    "#for i in range(1, len(allTopics)):\n",
    "#    for j in range(i):\n",
    "#        x = time.time()\n",
    "#        distance_matrix[i, j] = euclidean(allTopics[i], allTopics[j])\n",
    "#        similarity_matrix[i, j] = 1-cosine(allTopics[i], allTopics[j])\n",
    "#        similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "def build_vec_wtf(text, cls):\n",
    "    x = text\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space    \n",
    "    text = text.translate(translator).lower().split()\n",
    "    start = time.time()\n",
    "    text = [t for t in text if t in word2vec and not t in stopwords.words(\"english\")]\n",
    "    if cls == \"kmeans\":\n",
    "        num_clusters = math.ceil(np.sqrt(len(set(text))))\n",
    "        vecs = [word2vec[w] for w in text]\n",
    "        km = KMeans(n_clusters = num_clusters, random_state=3425, precompute_distances = True)\n",
    "        km.fit(vecs)\n",
    "        cluster_sizes = np.array([len(np.where(km.labels_ == i)[0]) for i in range(num_clusters)])\n",
    "        weights = cluster_sizes/sum(cluster_sizes)\n",
    "        return np.concatenate([[num_clusters], km.cluster_centers_.flatten(), weights])\n",
    "    elif cls == \"spectral\":\n",
    "        word_counter = Counter(text)\n",
    "        unique_text = list(word_counter.keys())\n",
    "        word_counts = list(word_counter.values())\n",
    "        if len(unique_text) < 3:\n",
    "            unique_text = unique_text*2\n",
    "            word_counts = word_counts*2\n",
    "        num_clusters = math.ceil(np.sqrt(len(unique_text)))\n",
    "        vecs = [word2vec[w] for w in unique_text]\n",
    "        sc = SpectralClustering(num_clusters, affinity='precomputed', random_state=3425,\n",
    "                                assign_labels = \"discretize\", n_init=100)\n",
    "        similarity_matrix = cosine_similarity(vecs)+1\n",
    "        sc.fit(similarity_matrix)\n",
    "        silhoutte_scores = silhouette_samples(2-similarity_matrix, sc.labels_, metric = 'precomputed')\n",
    "        non_empty_clusters = set(sc.labels_)\n",
    "        indices = [np.where(sc.labels_ == i)[0].tolist() if i in non_empty_clusters \n",
    "                                                           else [] for i in range(num_clusters)]\n",
    "        sh_means = [np.mean(np.take(silhoutte_scores, indices[i])) for i in non_empty_clusters]\n",
    "        sh_weights = np.array(sh_means/sum(sh_means))\n",
    "        sizes_weights = np.array([sum(np.take(word_counts, indices[i]))/wordSum for i in non_empty_clusters])\n",
    "        p = 0.2\n",
    "        weights = p*sh_weights+(1-p)*sizes_weights\n",
    "        centroids = np.array([np.mean(np.take(vecs, indices[i], axis = 0), axis = 0) for i in non_empty_clusters])\n",
    "        \n",
    "        return np.sum([np.multiply(centroids[i], weights[i]) for i in range(len(non_empty_clusters))], axis = 0)\n",
    "\n",
    "def build_vec_auto(text, cls, strategy):\n",
    "    x = text\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space    \n",
    "    text = text.translate(translator).lower().split()\n",
    "    start = time.time()\n",
    "    text = [t for t in text if t in word2vec and not t in stopwords.words(\"english\")]\n",
    "    if cls == \"kmeans\":\n",
    "        num_clusters = math.ceil(np.sqrt(len(set(text))))\n",
    "        vecs = [word2vec[w] for w in text]\n",
    "        km = KMeans(n_clusters = num_clusters, random_state=3425, precompute_distances = True)\n",
    "        km.fit(vecs)\n",
    "        cluster_sizes = np.array([len(np.where(km.labels_ == i)[0]) for i in range(num_clusters)])\n",
    "        weights = cluster_sizes/sum(cluster_sizes)\n",
    "        return np.concatenate([[num_clusters], km.cluster_centers_.flatten(), weights])\n",
    "    elif cls == \"spectral\":\n",
    "        if strategy == 0:\n",
    "            word_counter = Counter(text)\n",
    "            unique_text = list(word_counter.keys())\n",
    "            word_counts = list(word_counter.values())\n",
    "            if len(unique_text) < 3:\n",
    "                unique_text = unique_text*2\n",
    "                word_counts = word_counts*2\n",
    "            num_clusters = math.ceil(np.sqrt(len(unique_text)))\n",
    "            vecs = [word2vec[w] for w in unique_text]\n",
    "            sc = SpectralClustering(num_clusters, affinity='precomputed', random_state=3425,\n",
    "                                    assign_labels = \"discretize\", n_init=100)\n",
    "            similarity_matrix = cosine_similarity(vecs)+1\n",
    "            sc.fit(similarity_matrix)\n",
    "            silhoutte_scores = silhouette_samples(2-similarity_matrix, sc.labels_, metric = 'precomputed')\n",
    "            non_empty_clusters = list(set(sc.labels_))\n",
    "            indices = [np.where(sc.labels_ == i)[0].tolist() if i in non_empty_clusters \n",
    "                                                               else [] for i in range(num_clusters)]\n",
    "            sh_means = np.array([np.mean(np.take(silhoutte_scores, indices[i])) for i in non_empty_clusters])\n",
    "            pos_sh_means_indices = np.where(sh_means > 0)[0].tolist()\n",
    "            sh_means = np.take(sh_means, pos_sh_means_indices, axis = 0)\n",
    "            good_clusters = np.take(non_empty_clusters, pos_sh_means_indices)\n",
    "            sh_weights = np.array(sh_means/sum(sh_means))\n",
    "            \n",
    "            sizes = np.array([sum(np.take(word_counts, indices[i])) for i in good_clusters])\n",
    "            sizes_weights = sizes/sum(sizes)\n",
    "            p = 0.3\n",
    "            weights = p*sh_weights+(1-p)*sizes_weights\n",
    "            #print(\"{},{},{}\".format(sum(sh_weights), sum(sizes_weights), sum(weights)))\n",
    "            centroids = np.array([np.mean(np.take(vecs, indices[i], axis = 0), axis = 0) for i in good_clusters])\n",
    "            rep = np.concatenate([[len(good_clusters)], centroids.flatten(), sizes_weights])\n",
    "            return rep\n",
    "        elif strategy == 1:\n",
    "            word_counter = Counter(text)\n",
    "            unique_text = list(word_counter.keys())\n",
    "            word_counts = list(word_counter.values())\n",
    "            if len(unique_text) < 3:\n",
    "                unique_text = unique_text*2\n",
    "                word_counts = word_counts*2\n",
    "            num_clusters = math.ceil(np.sqrt(len(unique_text)))\n",
    "            vecs = [word2vec[w] for w in unique_text]\n",
    "            sc = SpectralClustering(num_clusters, affinity='precomputed', random_state=3425,\n",
    "                                    assign_labels = \"discretize\", n_init=100)\n",
    "            similarity_matrix = cosine_similarity(vecs)+1\n",
    "            sc.fit(similarity_matrix)\n",
    "            silhoutte_scores = silhouette_samples(2-similarity_matrix, sc.labels_, metric = 'precomputed')\n",
    "            silhoutte_contribution = np.multiply(word_counts, silhoutte_scores)\n",
    "            total_contribution = sum(silhoutte_contribution)\n",
    "            non_empty_clusters = list(set(sc.labels_))\n",
    "            indices = [np.where(sc.labels_ == i)[0].tolist() if i in non_empty_clusters \n",
    "                                                               else [] for i in range(num_clusters)]\n",
    "            \n",
    "            weights = np.array([sum(np.take(silhoutte_contribution, indices[i]))/total_contribution\n",
    "                                                                        for i in non_empty_clusters])\n",
    "            #print(\"{},{},{}\".format(sum(sh_weights), sum(sizes_weights), sum(final_weights)))\n",
    "            centroids = np.array([np.mean(np.take(vecs, indices[i], axis = 0), axis = 0) for i in non_empty_clusters])\n",
    "            rep = np.concatenate([[len(non_empty_clusters)], centroids.flatten(), weights])\n",
    "            #for i in range(len(weights)):\n",
    "             #   print(weights[i])\n",
    "              #  print(np.take(unique_text, indices[non_empty_clusters[i]]))\n",
    "            #print(\"-----\")\n",
    "            return rep\n",
    "        \n",
    "    \n",
    "\n",
    "def cluster_TSCD_auto(a, b):\n",
    "    def euclidean(a, b):\n",
    "        return np.sqrt(np.sum(np.square(a-b)))\n",
    "    start = time.time()\n",
    "    num_clusters_a, num_clusters_b = int(a[0]), int(b[0])\n",
    "    cut_a, cut_b = 1+300*num_clusters_a, 1+300*num_clusters_b\n",
    "    topics1, topics2 = a[1:cut_a].reshape(-1,300), b[1:cut_b].reshape(-1,300)\n",
    "    weights1, weights2 = a[cut_a:cut_a+num_clusters_a], b[cut_b:cut_b+num_clusters_b]\n",
    "    allTopics = np.concatenate([topics1,topics2])\n",
    "    d1 = np.concatenate([weights1,[0]*len(topics2)])\n",
    "    d2 = np.concatenate([[0]*len(topics1),weights2])\n",
    "    similarity_matrix = cosine_similarity(allTopics)\n",
    "    result = d1.T.dot(similarity_matrix).dot(d2)\n",
    "    vec1 = d1.T.dot(similarity_matrix).dot(d1)\n",
    "    vec2 = d2.T.dot(similarity_matrix).dot(d2)\n",
    "    result /= (np.sqrt(vec1)*np.sqrt(vec2))\n",
    "    #print(time.time()-start)\n",
    "    return np.clip(1-result, 0 , 1)\n",
    "\n",
    "def build_vec(text, num_clusters):\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "    text = text.translate(translator).lower().split()\n",
    "    vecs = [word2vec[w] for w in text if w in word2vec and not w in stopwords.words(\"english\")]\n",
    "    km = KMeans(n_clusters = num_clusters, random_state=3425, precompute_distances = True)\n",
    "    km.fit(vecs)\n",
    "    cluster_sizes = [len(np.where(km.labels_ == i)[0]) for i in range(num_clusters)]\n",
    "    return np.append(km.cluster_centers_.flatten(), np.array(cluster_sizes))\n",
    "\n",
    "def cluster_TSCD(a, b, num_clusters):\n",
    "    def euclidean(a, b):\n",
    "        return np.sqrt(np.sum(np.square(a-b)))\n",
    "    cut = 300*num_clusters\n",
    "    topics1, topics2 = a[0:cut].reshape(-1,300), b[0:cut].reshape(-1,300)\n",
    "    freq1, freq2 = a[cut:], b[cut:]\n",
    "    allTopics = np.concatenate([topics1,topics2])\n",
    "    \n",
    "    d1 = np.concatenate([freq1/sum(freq1),[0]*len(topics2)])\n",
    "    d2 = np.concatenate([[0]*len(topics1),freq2/sum(freq2)])\n",
    "    similarity_matrix = np.ones((len(allTopics), len(allTopics)), np.float64)\n",
    "    for i in range(1, len(allTopics)):\n",
    "        for j in range(i):\n",
    "            #distance_matrix[i, j] = euclidean(allTopics[i], allTopics[j])\n",
    "            similarity_matrix[i, j] = 1-cosine(allTopics[i], allTopics[j])\n",
    "            similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "    result = d1.T.dot(similarity_matrix).dot(d2)\n",
    "    vec1 = d1.T.dot(similarity_matrix).dot(d1)\n",
    "    vec2 = d2.T.dot(similarity_matrix).dot(d2)\n",
    "    result /= (np.sqrt(vec1)*np.sqrt(vec2))\n",
    "    return np.clip(1-result, 0, 1)\n",
    "\n",
    "def cluster_TSCD_raw(text1, text2, num_clusters):\n",
    "    return cluster_TSCD(build_vec(text1, num_clusters), build_vec(text2, num_clusters), num_clusters)\n",
    "\n",
    "def cluster_TSCD_raw_auto(text1, text2, cls, strategy):\n",
    "    return cluster_TSCD_auto(build_vec_auto(text1, cls, strategy), build_vec_auto(text2, cls, strategy))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "num_clusters = 2\n",
    "#vecs = [build_vec_auto(t, \"kmeans\") for t in texts]\n",
    "#print(time.time() - start)\n",
    "start = time.time()\n",
    "#vecs = [build_vec_auto(t, \"spectral\", 1) for t in texts]\n",
    "#print(time.time() - start)\n",
    "#for i in range(1000):\n",
    "    #cluster_TSCD_auto(vecs[10], vecs[11])\n",
    "    #print(cluster_WMDdistance(texts[0], texts[i], num_clusters))\n",
    "#start = time.time()\n",
    "#for i in range(100):\n",
    "    #build_vec_auto(texts[-1], \"spectral\")\n",
    "#print(time.time() - start)\n",
    "start = time.time()\n",
    "#for i in range(100):\n",
    "    #build_vec_auto(texts[-1], \"kmeans\")\n",
    "#print(time.time() - start)\n",
    "#print(time.time() - start)\n",
    "#cluster_TSCD_auto(vecs[10], vecs[11])\n",
    "start = time.time()\n",
    "print(\"score: {}\".format(cluster_TSCD_raw_auto(texts[-1], texts[-1], \"spectral\", 0)))\n",
    "print(\"time:{}\".format(time.time() - start))\n",
    "start = time.time()\n",
    "print(\"score: {}\".format(cluster_TSCD_raw_auto(texts[-1], texts[-1], \"kmeans\", 0)))\n",
    "print(\"time:{}\".format(time.time() - start))\n",
    "#print(cluster_WMDdistance(texts[3], texts[4], 3))\n",
    "#print(vecs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustered WMD distance. \n",
    "Takes raw text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def cluster_WMDdistance(text1, text2, num_clusters):\n",
    "    \n",
    "    def build_rep(text, num_clusters):\n",
    "        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "        text = text.translate(translator).lower().split()\n",
    "        vecs = [word2vec[w] for w in text if w in word2vec]\n",
    "        km = KMeans(n_clusters = num_clusters, random_state=3425)\n",
    "        km.fit(vecs)\n",
    "        cluster_sizes = [len(np.where(km.labels_ == i)[0]) for i in range(num_clusters)]\n",
    "        return km.cluster_centers_, np.array(cluster_sizes)\n",
    "        \n",
    "    def euclidean(a, b):\n",
    "        return np.sqrt(np.sum(np.square(a-b)))\n",
    "\n",
    "    topics1, freq1 = build_rep(text1, num_clusters)\n",
    "    topics2, freq2 = build_rep(text2, num_clusters)\n",
    "    allTopics = np.concatenate([topics1,topics2])\n",
    "    d1 = np.concatenate([freq1/sum(freq1),[0]*len(topics2)])\n",
    "    d2 = np.concatenate([[0]*len(topics1),freq2/sum(freq2)]) \n",
    "    distance_matrix = [ [euclidean(allTopics[i], allTopics[j]) \n",
    "                        for j in range(len(allTopics))] \n",
    "                           for i in range(len(allTopics)) ]\n",
    "    #print(distance_matrix)\n",
    "    return emd(d1, d2, np.array(distance_matrix))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word average distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wad(text):\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "    text = text.translate(translator).lower().split()\n",
    "    text = [t for t in text if not t in stopwords.words(\"english\")]\n",
    "    return np.mean([word2vec[w] for w in text if w in word2vec], axis=0)\n",
    "\n",
    "def wordcount(text):\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "    text = text.translate(translator).lower().split()\n",
    "    text = [t for t in text if t in word2vec]\n",
    "    return len(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "cats = ['comp.graphics',\n",
    "        'sci.crypt',\n",
    "        'rec.autos',\n",
    "        'sci.space',\n",
    "        'talk.politics.guns',\n",
    "        'rec.sport.baseball']\n",
    "news20_train = fetch_20newsgroups(subset='train',categories=cats)\n",
    "news20_test = fetch_20newsgroups(subset='test',categories=cats)\n",
    "\n",
    "train_X = news20_train.data\n",
    "test_X = news20_test.data\n",
    "train_y = news20_train.target\n",
    "test_y = news20_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices_train = [i for i in range(len(train_X)) if wordcount(train_X[i]) > 100 ]\n",
    "indices_test = [i for i in range(len(test_X)) if wordcount(test_X[i]) > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing\n",
      "1874.754192829132\n"
     ]
    }
   ],
   "source": [
    "indices_train = [i for i in range(len(train_X)) if wordcount(train_X[i]) > 100 ]\n",
    "indices_test = [i for i in range(len(test_X)) if wordcount(test_X[i]) > 100]\n",
    "\n",
    "l_train_X = np.take(train_X, indices_train)\n",
    "l_test_X = np.take(test_X, indices_test)\n",
    "l_train_y = np.take(train_y, indices_train)\n",
    "l_test_y = np.take(test_y, indices_test)\n",
    "print(\"vectorizing\")\n",
    "l_train_vecs = [ build_vec_auto(t,\"spectral\", 0) for t in l_train_X]\n",
    "l_test_vecs = [build_vec_auto(t, \"spectral\", 0) for t in l_test_X]\n",
    "\n",
    "max_len = np.max(np.concatenate(([len(a) for a in l_train_vecs], [len(a) for a in l_test_vecs])))\n",
    "l_train_vecs = np.asarray([np.pad(a, (0, max_len - len(a)), 'constant', constant_values=0) for a in l_train_vecs])\n",
    "l_test_vecs = np.asarray([np.pad(a, (0, max_len - len(a)), 'constant', constant_values=0) for a in l_test_vecs])\n",
    "#train_vecs = [wad(t) for t in news20_train.data]\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGIBJREFUeJzt3X+UXOV93/H3xwIDAYLALAQkkcVY\nrn9wYuFsBAn5QcBJ+ZFE+BxIoI4RPjiyY4jt2G34cdKC0+DgHAyx65ZUGGKRYEDF2KiAYxNAUFoD\nXrD4IYOLAsJaJEtrgwCZmEbi0z/us2ZYze7M7s7ujK4+r3P2zL3PfWbme6+0n3n2mTtzZZuIiKiv\nN3S7gIiImF4J+oiImkvQR0TUXII+IqLmEvQRETWXoI+IqLkEffyUpNWSjul2Hb1A0gWSvjjO9jMl\n3TuTNXVaq32Q9HVJi2eyppgeCfqdhKS1kt4zqu11v+i232l7ZYvH6ZdkSbtMU6k9wfanbX8QOrPP\n5fj3T7UuSRdJ+ocJ9r9oMs9l+wTbyyZz3+gtCfroKXV/AYnohgR9/FTjqF/SQkmDkl6UtFHSZaXb\nPeV2s6Qtkn5Z0hsk/bmkZyRtknSNpH0aHveMsu1Hkv7jqOe5SNKNkv5B0ovAmeW5vyVps6QNkr4g\n6Y0Nj2dJH5H0pKSXJP1nSYeV+7woaXlj/1H7+IykXyzLf1ge6x1l/YOSvtZQ18jIebt9bni8SyU9\nL+lpSSe0eZxPkvSdUuu6xhG3pGMkDTX7d5F0PHAB8AeljofL9oMlrZD0nKQ1kv6onTpee3j9F0kv\nSHpC0nENG1ZKGvmr5kxJ9461v2X7U+Xf42lJ75tADTHNEvQxls8Bn7P9s8BhwPLS/uvldrbtvWx/\nCziz/Pwm8GZgL+ALACVE/xvwPuAgYB9gzqjnWgTcCMwGrgW2AX8K7A/8MnAc8JFR9zke+EXgKODP\ngKXlOeYBhwOnj7FfdwPHNOzLU8BvNKzf3eQ+zfYZ4Ejge6XOvwaukqRmT2q73/basvpj4IyyvycB\nfyzp5DHqbXyMfwQ+DdxQ6nhX2XQdMAQcDJwCfHoksG1fZPuicR72SKpjsD9wIXCTpP3G6bvd/kra\nE/g8cILtvYFfAVa12p+YOQn6ncvXyih5s6TNVAE8ln8F3iJpf9tbbN83Tt/3AZfZfsr2FuB84LQy\nDXMK8D9t32v7/wH/CRj9BUvfsv0126/a/hfbD9q+z/bWEo7/ndfCeMRnbL9oezXwGPDN8vwvAF8H\njhij1rsbHuvXgL9qWP8Nmgf9WJ6xfaXtbcAyqheyA1vdyfZK24+W/X2EKqhH719bJM0DfhU41/ZP\nbK8Cvgi8v82H2AT8je1/tX0DVZCfNEbf8fb3VeBwSXvY3lD+XaJHJOh3Lifbnj3yw/aj5EZnAW8F\nnpD0bUm/M07fg4FnGtafAXahCoGDgXUjG2y/DPxo1P3XNa5IequkWyT9oEznfJpqFNloY8PyvzRZ\n32uMWu8Gfk3SzwGzgBuAo8sbpfswsZHoD0YWyn4xzvP+lKQjJd0laVjSC8CH2X7/2nUw8Jztlxra\nnmH7v5rG8qxf/82Gz5THbKbp/tr+MfAHVPuxQdKtkt7W5vPHDEjQR1O2n7R9OnAA8BngxvInerOv\nO10P/HzD+iHAVqrw3QDMHdkgaQ/gTaOfbtT6FcATwPwydXQB0HRKZKJsrwFeBj4K3FMC8gfAEuBe\n2682u1snnrvBl4EVwDzb+wB/y2v792PgZ0Y6SpoF9I1Ty3pgP0l7N7QdAjzbZi1zRk03HVIec0Js\nf8P2b1GN8p8ArpzoY8T0SdBHU+WNyr4SfJtL8zZgmOrP9Dc3dL8O+FNJh0rai9fmkbdSzb3/rqRf\nKW+QforWob038CKwpYwM/7hjO1a5GziH16ZpVo5aH63ZPk/F3lSj8J9IWgj8u4Zt/xfYvbxhuyvw\n58BuDds3Av2S3gBgex3wf4C/krS7pF+g+mvs2jZrOQD4qKRdJZ0KvB24bSI7I+lASb9XBgKvAFuo\n/q9Ej0jQx1iOB1ZL2kL1xuxpZQ74ZeBi4H+Xuf6jgKuBv6c6O+Vp4CfAnwCUudo/Aa6nGt2/RDUv\n/Mo4z/3vqcLvJaqR4Q0d3re7qcL2njHWX2eMfZ6KjwB/IeklqvcsRt7oprzH8BGqefZnqUb4jWfh\n/I9y+yNJD5Xl04F+qpH4V4ELbd/eZi33A/OBH1Lt4ym2R0+ttfIG4JPl+Z+jer9hvGnBmGHKhUdi\nJpUR/2aqaZmnu11PxM4gI/qYdpJ+V9LPlD/tLwUeBdZ2t6qInUeCPmbCIqo/69dTTROc5vwpGTFj\nMnUTEVFzGdFHRNRcT3yB1P777+/+/v5ulxERsUN58MEHf2i7r1W/ngj6/v5+BgcHu11GRMQORdIz\nrXtl6iYiovYS9BERNZegj4iouQR9RETNJegjImouQR8RUXMJ+oiImkvQR0TUXII+IqLmeuKTsTuT\n/vNufd362kvGug5zRERnZEQfEVFzbQe9pFmSviPplrJ+qKT7JT0p6YZyPVAk7VbW15Tt/dNTekRE\ntGMiI/qPAY83rH8GuNz2fOB5qgsSU26ft/0W4PLSLyIiuqStoJc0FziJ6oLFSBJwLHBj6bIMOLks\nLyrrlO3Hlf4REdEF7Y7o/wb4M+DVsv4mYLPtrWV9CJhTlucA6wDK9hdK/9eRtETSoKTB4eHhSZYf\nERGttAx6Sb8DbLL9YGNzk65uY9trDfZS2wO2B/r6Wn5vfkRETFI7p1ceDfyepBOB3YGfpRrhz5a0\nSxm1z6W68DNUo/t5wJCkXYB9gOc6XnlERLSl5Yje9vm259ruB04D7rT9PuAu4JTSbTFwc1leUdYp\n2+90rkAeEdE1UzmP/lzgE5LWUM3BX1XarwLeVNo/AZw3tRIjImIqJvTJWNsrgZVl+SlgYZM+PwFO\n7UBtERHRAflkbEREzSXoIyJqLl9q1oPyxWcR0UkZ0UdE1FyCPiKi5hL0ERE1l6CPiKi5BH1ERM0l\n6CMiai5BHxFRcwn6iIiaS9BHRNRcgj4iouYS9BERNZegj4iouQR9RETNtXNx8N0lPSDpYUmrJX2q\ntH9J0tOSVpWfBaVdkj4vaY2kRyS9e7p3IiIixtbO1xS/Ahxre4ukXYF7JX29bPsPtm8c1f8EYH75\nORK4otxGREQXtHNxcNveUlZ3LT/jXex7EXBNud99wGxJB0291IiImIy25uglzZK0CtgE3G77/rLp\n4jI9c7mk3UrbHGBdw92HStvox1wiaVDS4PDw8BR2ISIixtNW0NveZnsBMBdYKOlw4HzgbcAvAfsB\n55buavYQTR5zqe0B2wN9fX2TKj4iIlqb0Fk3tjcDK4HjbW8o0zOvAH8HLCzdhoB5DXebC6zvQK0R\nETEJ7Zx10ydpdlneA3gP8MTIvLskAScDj5W7rADOKGffHAW8YHvDtFQfEREttXPWzUHAMkmzqF4Y\nltu+RdKdkvqopmpWAR8u/W8DTgTWAC8DH+h82RER0a6WQW/7EeCIJu3HjtHfwNlTLy0iIjohn4yN\niKi5BH1ERM0l6CMiai5BHxFRcwn6iIiaS9BHRNRcgj4iouYS9BERNZegj4iouQR9RETNJegjImou\nQR8RUXMJ+oiImkvQR0TUXII+IqLmEvQRETXXzqUEd5f0gKSHJa2W9KnSfqik+yU9KekGSW8s7buV\n9TVle//07kJERIynnRH9K8Cxtt8FLACOL9eC/Qxwue35wPPAWaX/WcDztt8CXF76RUREl7QMele2\nlNVdy4+BY4EbS/syqguEAywq65Ttx5ULiEdERBe0NUcvaZakVcAm4Hbgn4HNtreWLkPAnLI8B1gH\nULa/ALypk0VHRET72gp629tsLwDmAguBtzfrVm6bjd49ukHSEkmDkgaHh4fbrTciIiZoQmfd2N4M\nrASOAmZL2qVsmgusL8tDwDyAsn0f4Lkmj7XU9oDtgb6+vslVHxERLbVz1k2fpNlleQ/gPcDjwF3A\nKaXbYuDmsryirFO232l7uxF9RETMjF1ad+EgYJmkWVQvDMtt3yLpu8D1kv4S+A5wVel/FfD3ktZQ\njeRPm4a6IyKiTS2D3vYjwBFN2p+imq8f3f4T4NSOVBcREVOWT8ZGRNRcgj4iouYS9BERNZegj4io\nuQR9RETNJegjImouQR8RUXPtfGAq2tR/3q3bta295KQuVBIR8ZqM6CMiai5BHxFRcwn6iIiaS9BH\nRNRcgj4iouYS9BERNZegj4iouQR9RETNJegjImqu5SdjJc0DrgF+DngVWGr7c5IuAv4IGC5dL7B9\nW7nP+cBZwDbgo7a/MQ2177TyCdyImIh2vgJhK/BJ2w9J2ht4UNLtZdvlti9t7CzpHVTXiX0ncDDw\nT5LeantbJwuPiIj2tJy6sb3B9kNl+SXgcWDOOHdZBFxv+xXbTwNraHJt2YiImBkTmqOX1E91ofD7\nS9M5kh6RdLWkfUvbHGBdw92GaPLCIGmJpEFJg8PDw6M3R0REh7Qd9JL2Ar4CfNz2i8AVwGHAAmAD\n8NmRrk3u7u0a7KW2B2wP9PX1TbjwiIhoT1tBL2lXqpC/1vZNALY32t5m+1XgSl6bnhkC5jXcfS6w\nvnMlR0TERLQMekkCrgIet31ZQ/tBDd3eCzxWllcAp0naTdKhwHzggc6VHBERE9HOWTdHA+8HHpW0\nqrRdAJwuaQHVtMxa4EMAtldLWg58l+qMnbNzxk1ERPe0DHrb99J83v22ce5zMXDxFOqKiIgOySdj\nIyJqLkEfEVFzCfqIiJpL0EdE1FyCPiKi5hL0ERE1l6CPiKi5BH1ERM0l6CMiai5BHxFRcwn6iIia\nS9BHRNRcgj4iouYS9BERNZegj4iouQR9RETNtXMpwXmS7pL0uKTVkj5W2veTdLukJ8vtvqVdkj4v\naY2kRyS9e7p3IiIixtbOiH4r8EnbbweOAs6W9A7gPOAO2/OBO8o6wAlU14mdDywBruh41RER0baW\nQW97g+2HyvJLwOPAHGARsKx0WwacXJYXAde4ch8we9SFxCMiYgZNaI5eUj9wBHA/cKDtDVC9GAAH\nlG5zgHUNdxsqbaMfa4mkQUmDw8PDE688IiLa0nbQS9oL+Arwcdsvjte1SZu3a7CX2h6wPdDX19du\nGRERMUFtBb2kXalC/lrbN5XmjSNTMuV2U2kfAuY13H0usL4z5UZExES1c9aNgKuAx21f1rBpBbC4\nLC8Gbm5oP6OcfXMU8MLIFE9ERMy8XdroczTwfuBRSatK2wXAJcBySWcB3wdOLdtuA04E1gAvAx/o\naMURETEhLYPe9r00n3cHOK5JfwNnT7GuiIjokHwyNiKi5hL0ERE1l6CPiKi5BH1ERM0l6CMiai5B\nHxFRcwn6iIiaS9BHRNRcgj4iouYS9BERNZegj4iouQR9RETNJegjImouQR8RUXMJ+oiImkvQR0TU\nXDuXErxa0iZJjzW0XSTpWUmrys+JDdvOl7RG0vck/dvpKjwiItrTzqUEvwR8AbhmVPvlti9tbJD0\nDuA04J3AwcA/SXqr7W0dqDUmoP+8W1+3vvaSk7pUSUR0W8sRve17gOfafLxFwPW2X7H9NNV1YxdO\nob6IiJiiqczRnyPpkTK1s29pmwOsa+gzVNq2I2mJpEFJg8PDw1MoIyIixjPZoL8COAxYAGwAPlva\nm11E3M0ewPZS2wO2B/r6+iZZRkREtDKpoLe90fY2268CV/La9MwQMK+h61xg/dRKjIiIqZhU0Es6\nqGH1vcDIGTkrgNMk7SbpUGA+8MDUSoyIiKloedaNpOuAY4D9JQ0BFwLHSFpANS2zFvgQgO3VkpYD\n3wW2AmfnjJuIiO5qGfS2T2/SfNU4/S8GLp5KURER0TntnEcfO4mcex9RT/kKhIiImkvQR0TUXII+\nIqLmEvQRETWXoI+IqLkEfUREzSXoIyJqLkEfEVFzCfqIiJpL0EdE1Fy+AmEC8hUBEbEjyog+IqLm\nEvQRETWXoI+IqLkEfUREzbUMeklXS9ok6bGGtv0k3S7pyXK7b2mXpM9LWiPpEUnvns7iIyKitXZG\n9F8Cjh/Vdh5wh+35wB1lHeAEquvEzgeWAFd0psyIiJislkFv+x7guVHNi4BlZXkZcHJD+zWu3AfM\nHnUh8YiImGGTnaM/0PYGgHJ7QGmfA6xr6DdU2rYjaYmkQUmDw8PDkywjIiJa6fSbsWrS5mYdbS+1\nPWB7oK+vr8NlRETEiMkG/caRKZlyu6m0DwHzGvrNBdZPvryIiJiqyQb9CmBxWV4M3NzQfkY5++Yo\n4IWRKZ6IiOiOlt91I+k64Bhgf0lDwIXAJcBySWcB3wdOLd1vA04E1gAvAx+YhpojImICWga97dPH\n2HRck74Gzp5qURER0Tn5ZGxERM0l6CMiai5BHxFRcwn6iIiaS9BHRNRcgj4iouYS9BERNZegj4io\nuQR9RETNJegjImouQR8RUXMJ+oiImkvQR0TUXMtvr4xo1H/era9bX3vJSV2qJCLalRF9RETNJegj\nImpuSlM3ktYCLwHbgK22ByTtB9wA9ANrgd+3/fzUyoyIiMnqxIj+N20vsD1Q1s8D7rA9H7ijrEdE\nRJdMx5uxi6iuMQuwDFgJnDsNzxM9aPSbtZA3bCO6baojegPflPSgpCWl7UDbGwDK7QFTfI6IiJiC\nqY7oj7a9XtIBwO2Snmj3juWFYQnAIYccMsUyIiJiLFMa0dteX243AV8FFgIbJR0EUG43jXHfpbYH\nbA/09fVNpYyIiBjHpINe0p6S9h5ZBn4beAxYASwu3RYDN0+1yIiImLypTN0cCHxV0sjjfNn2P0r6\nNrBc0lnA94FTp15mRERM1qSD3vZTwLuatP8IOG4qRUVEROfkk7ERETWXoI+IqLkEfUREzSXoIyJq\nLt9HX+R71iOirhL0MePyohoxszJ1ExFRcwn6iIiay9RN9KRM70R0Tkb0ERE1l6CPiKi5TN3EDivT\nOxHtyYg+IqLmEvQRETWXoI+IqLnM0UdtjZ7Dh8zjx84pQR8xDfJGcfSSaQt6SccDnwNmAV+0fcl0\nPVcr+aWLsUzm/8ZM/X/K/9volGkJekmzgP8K/BYwBHxb0grb352O54vY0XRqWqkTLwZ5Qam/6RrR\nLwTWlOvKIul6YBHQ8aDPPGxEb5qOF6FOPc7ox9jRnmeiZLvzDyqdAhxv+4Nl/f3AkbbPaeizBFhS\nVv8N8L2OFzJx+wM/7HYRk5C6Z1bqnlk7at0w/bX/vO2+Vp2ma0SvJm2ve0WxvRRYOk3PPymSBm0P\ndLuOiUrdMyt1z6wdtW7ondqn6zz6IWBew/pcYP00PVdERIxjuoL+28B8SYdKeiNwGrBimp4rIiLG\nMS1TN7a3SjoH+AbV6ZVX2149Hc/VYT01lTQBqXtmpe6ZtaPWDT1S+7S8GRsREb0j33UTEVFzCfqI\niJpL0AOS1kp6VNIqSYPdrmc8kq6WtEnSYw1t+0m6XdKT5XbfbtbYzBh1XyTp2XLcV0k6sZs1NiNp\nnqS7JD0uabWkj5X2nj7m49Td08dc0u6SHpD0cKn7U6X9UEn3l+N9QznJo2eMU/eXJD3dcLwXdKW+\nzNFXQQ8M2O75D2VI+nVgC3CN7cNL218Dz9m+RNJ5wL62z+1mnaONUfdFwBbbl3aztvFIOgg4yPZD\nkvYGHgROBs6kh4/5OHX/Pj18zCUJ2NP2Fkm7AvcCHwM+Adxk+3pJfws8bPuKbtbaaJy6PwzcYvvG\nbtaXEf0OxvY9wHOjmhcBy8ryMqpf6J4yRt09z/YG2w+V5ZeAx4E59PgxH6funubKlrK6a/kxcCww\nEpa9eLzHqrsnJOgrBr4p6cHy1Qw7mgNtb4DqFxw4oMv1TMQ5kh4pUzs9Nf0xmqR+4AjgfnagYz6q\nbujxYy5plqRVwCbgduCfgc22t5YuQ/Tgi9boum2PHO+Ly/G+XNJu3agtQV852va7gROAs8s0Q0y/\nK4DDgAXABuCz3S1nbJL2Ar4CfNz2i92up11N6u75Y257m+0FVJ+oXwi8vVm3ma2qtdF1SzocOB94\nG/BLwH5AV6b3EvSA7fXldhPwVar/XDuSjWVOdmRudlOX62mL7Y3ll+NV4Ep69LiXOdevANfavqk0\n9/wxb1b3jnLMAWxvBlYCRwGzJY18wLOnv1Kloe7jyxSabb8C/B1dOt47fdBL2rO8WYWkPYHfBh4b\n/149ZwWwuCwvBm7uYi1tGwnK4r304HEvb7JdBTxu+7KGTT19zMequ9ePuaQ+SbPL8h7Ae6jeX7gL\nOKV068Xj3azuJxoGA6J6X6Erx3unP+tG0pupRvFQfSXEl21f3MWSxiXpOuAYqq8/3QhcCHwNWA4c\nAnwfONV2T73xOUbdx1BNIRhYC3xoZN67V0j6VeB/AY8Cr5bmC6jmu3v2mI9T9+n08DGX9AtUb7bO\nohqILrf9F+X39Hqq6Y/vAH9YRsk9YZy67wT6qL7RdxXw4YY3bWeuvp096CMi6m6nn7qJiKi7BH1E\nRM0l6CMiai5BHxFRcwn6iIiaS9BHRNRcgj4ioub+P0BrFJapKDR2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b15cd91d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = [t[0] for  t in l_train_vecs]\n",
    "plt.hist(a, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram with 'auto' bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(BBCNews.data, BBCNews.label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started KNN \n",
      "3.199345111846924\n",
      "started predicting\n",
      "0.842654735272\n",
      "464.11269998550415\n"
     ]
    }
   ],
   "source": [
    "print(\"started KNN \")\n",
    "#train_vecs_wad = [ wad(t) for t in train_X]\n",
    "#test_vecs_wad = [ wad(t) for t in test_X]\n",
    "start = time.time()\n",
    "\n",
    "#nbrs = GridSearchCV(KNeighborsClassifier(algorithm='ball_tree'), cv = None, \n",
    "     #               param_grid={\"n_neighbors\": range(5,25,5)})\n",
    "#nbrs = GridSearchCV(KNeighborsClassifier(algorithm='ball_tree', \n",
    " #                           metric=lambda a,b: cluster_TSCD_auto(a, b)), cv = None,\n",
    "  #                         param_grid={\"n_neighbors\": range(5,25,5)})\n",
    "nbrs = KNeighborsClassifier(algorithm='ball_tree', n_neighbors = 10,\n",
    "                            metric=lambda a,b: cluster_TSCD_auto(a, b))\n",
    "nbrs.fit(l_train_vecs, l_train_y)\n",
    "print(time.time() - start)\n",
    "start = time.time()\n",
    "#test_vecs = [build_vec(t, num_clusters) for t in news20_test.data]\n",
    "\n",
    "print(\"started predicting\")\n",
    "start = time.time()\n",
    "predicted = nbrs.predict(l_test_vecs)\n",
    "print(accuracy_score(predicted, l_test_y))\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.93      0.75        27\n",
      "          1       0.78      1.00      0.88        14\n",
      "          2       0.82      1.00      0.90        18\n",
      "          3       0.94      0.62      0.74        47\n",
      "          4       0.91      0.91      0.91        34\n",
      "          5       0.97      0.89      0.93        63\n",
      "\n",
      "avg / total       0.88      0.85      0.85       203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mt.classification_report(predicted, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.97      0.83        30\n",
      "          1       0.83      1.00      0.91        15\n",
      "          2       0.86      1.00      0.93        19\n",
      "          3       0.87      0.66      0.75        41\n",
      "          4       0.88      0.94      0.91        32\n",
      "          5       0.95      0.83      0.89        66\n",
      "\n",
      "avg / total       0.87      0.86      0.86       203\n",
      "\n",
      "-0.0411658\n",
      "0.00647455\n",
      "0.0169582\n",
      "0.0299543\n",
      "0.000656661\n",
      "0.0247794\n",
      "0.0173201\n",
      "0.0424855\n",
      "0.0404512\n",
      "0.0154711\n",
      "0.0216071\n",
      "0.0170083\n",
      "0.0148304\n",
      "-0.0197079\n",
      "0.0303551\n",
      "0.00789397\n",
      "-0.0116723\n",
      "0.00552831\n",
      "0.0272732\n",
      "-0.0160297\n",
      "-0.00865095\n",
      "-0.00385721\n",
      "0.024471\n",
      "0.0144022\n",
      "0.0263105\n",
      "0.0351309\n",
      "0.0353078\n",
      "0.0577167\n",
      "0.0270269\n",
      "0.0153471\n",
      "0.0168146\n",
      "0.0182642\n",
      "0.0339697\n",
      "0.0403333\n",
      "0.000163791\n",
      "-0.01483\n",
      "0.0225381\n",
      "0.0347577\n",
      "0.0409232\n",
      "0.00787568\n"
     ]
    }
   ],
   "source": [
    "print(mt.classification_report(predicted, test_y))\n",
    "for i, t in enumerate(test_vecs):\n",
    "    if test_y[i] == 0:\n",
    "        print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,1,1):\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([0,1,1])\n",
    "x.dot(np.array([[1,0,1],[0,1,1],[0,1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
